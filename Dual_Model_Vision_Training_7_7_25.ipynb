{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/acredsfan/autonomous_mower/blob/main/Dual_Model_Vision_Training_7_7_25.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cLD-8eVnL364",
    "outputId": "adba3441-7479-4a4e-b07c-92212d9072cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.163 üöÄ Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (NVIDIA A100-SXM4-40GB, 40507MiB)\n",
      "Setup complete ‚úÖ (12 CPUs, 83.5 GB RAM, 44.6/235.7 GB disk)\n"
     ]
    }
   ],
   "source": [
    "# @title Cell 1: Final Combined Installation and Verification\n",
    "# This is the ONLY cell you need for installations.\n",
    "\n",
    "# 1. Uninstall to ensure a clean slate\n",
    "print(\"Uninstalling all relevant libraries...\")\n",
    "!pip uninstall -y torch torchvision torchaudio ultralytics fiftyone tflite-runtime roboflow datasets\n",
    "\n",
    "# 2. Install GPU-enabled PyTorch\n",
    "print(\"\\nInstalling GPU-enabled PyTorch...\")\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
    "\n",
    "# 3. Install all other dependencies in one go\n",
    "print(\"\\nInstalling Ultralytics and other libraries...\")\n",
    "!pip install ultralytics roboflow fiftyone datasets tflite-runtime --quiet\n",
    "\n",
    "# 4. Verify the installation\n",
    "print(\"\\nVerifying the environment...\")\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Is CUDA available? {torch.cuda.is_available()}\")\n",
    "\n",
    "print(\"\\nRunning Ultralytics checks:\")\n",
    "import ultralytics\n",
    "ultralytics.checks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_bKiBYGHm_er"
   },
   "outputs": [],
   "source": [
    "# @title Cell 2: Class Setup and Directory Initialization\n",
    "# ----------------------------\n",
    "# CLASS SETUP & DIRECTORY INIT\n",
    "# ----------------------------\n",
    "import os\n",
    "MASTER_CLASS_LIST = [\"grass\", \"dirt\", \"sand\", \"mulch\", \"pavement\", \"concrete\", \"gravel\", \"tree\", \"shrub\", \"flower\", \"planter\", \"stump\", \"rock\", \"hill\", \"water_feature\", \"ditch\", \"pool\", \"lake\", \"river\", \"fountain\", \"waterfall\", \"field\", \"curb\", \"edging\", \"fence\", \"gate\", \"retaining_wall\", \"railing\", \"bench\", \"bridge\", \"stairs\", \"path\", \"sign\", \"pole\", \"lamp_post\", \"streetlight\", \"traffic_light\", \"person\", \"animal\", \"dog\", \"cat\", \"bicycle\", \"toy\", \"tool\", \"hose\", \"sprinkler\", \"swing_set\", \"slide\", \"sandbox\", \"trampoline\", \"furniture\", \"decoration\", \"vehicle\", \"car\", \"bus\", \"truck\", \"mailbox\", \"trash_bin\", \"recycling_bin\"]\n",
    "master_index = {name: idx for idx, name in enumerate(MASTER_CLASS_LIST)}\n",
    "BASE_DIR = \"/content/mower_dataset\"\n",
    "for split in [\"train\", \"val\"]:\n",
    "    os.makedirs(f\"{BASE_DIR}/images/{split}\", exist_ok=True)\n",
    "    os.makedirs(f\"{BASE_DIR}/labels/{split}\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LyYiEQyeCYV2",
    "outputId": "1478bfcf-59ad-4a6b-bccc-66aece659360"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "kaggle.json copied from Google Drive.\n"
     ]
    }
   ],
   "source": [
    "# @title Cell 3: Kaggle setup for COCO\n",
    "from google.colab import drive\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Define the path to kaggle.json in your Google Drive\n",
    "kaggle_drive_path = \"/content/drive/MyDrive/kaggle.json\"\n",
    "kaggle_colab_path = \"/root/.kaggle/kaggle.json\"\n",
    "\n",
    "# Create the .kaggle directory if it doesn't exist\n",
    "if not os.path.exists(\"/root/.kaggle\"):\n",
    "    os.makedirs(\"/root/.kaggle\")\n",
    "\n",
    "# Check if the file exists in Google Drive before copying\n",
    "if os.path.exists(kaggle_drive_path):\n",
    "    shutil.copy(kaggle_drive_path, kaggle_colab_path)\n",
    "    os.chmod(kaggle_colab_path, 0o600)\n",
    "    print(\"kaggle.json copied from Google Drive.\")\n",
    "else:\n",
    "    print(f\"Error: {kaggle_drive_path} not found in your Google Drive.\")\n",
    "    print(\"Please upload kaggle.json to the root of your MyDrive or manually upload it.\")\n",
    "    # Fallback to manual upload if the file is not in Drive\n",
    "    from google.colab import files\n",
    "    uploaded = files.upload()  # upload kaggle.json\n",
    "    for fn in uploaded.keys():\n",
    "        shutil.move(fn, kaggle_colab_path)\n",
    "    os.chmod(kaggle_colab_path, 0o600)\n",
    "    print(\"kaggle.json uploaded manually.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MtzFimoOCbXK",
    "outputId": "014c2c4a-c865-4b8e-9608-bd27f356ffea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/paragmraw/coco-2017-dataset-yolov8-format\n",
      "License(s): CC0-1.0\n",
      "Downloading coco-2017-dataset-yolov8-format.zip to /content\n",
      " 99% 5.81G/5.86G [00:13<00:00, 798MB/s]\n",
      "100% 5.86G/5.86G [00:13<00:00, 456MB/s]\n"
     ]
    }
   ],
   "source": [
    "# @title Cell 4: Download COCO 2017 (YOLOv8 format from Kaggle)\n",
    "!kaggle datasets download -d paragmraw/coco-2017-dataset-yolov8-format -p /content\n",
    "!unzip -q /content/coco-2017-dataset-yolov8-format.zip -d /content/coco2017\n",
    "print(\"COCO 2017 dataset downloaded and extracted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CF4n-PzsCeaq"
   },
   "outputs": [],
   "source": [
    "# @title Cell 5: COCO mapping and filtering\n",
    "COCO_TO_MASTER = {\n",
    "    0: 36,    # person\n",
    "    1: 40,    # bicycle\n",
    "    2: 51,    # car\n",
    "    3: 50,    # motorcycle -> vehicle\n",
    "    5: 52,    # bus\n",
    "    7: 53,    # truck\n",
    "    15: 39,   # cat\n",
    "    16: 38,   # dog\n",
    "    57: 48,   # chair -> furniture\n",
    "    60: 48,   # dining table -> furniture\n",
    "    58: 9,    # potted plant -> planter\n",
    "    77: 42,   # teddy bear -> toy\n",
    "}\n",
    "COCO_IMG_LIMIT = None  # Adjust for RAM/time; set to None for full dataset\n",
    "\n",
    "for split in [\"train\", \"val\"]:\n",
    "    img_dir = f\"/content/coco2017/{split}/images\"\n",
    "    lbl_dir = f\"/content/coco2017/{split}/labels\"\n",
    "    imgs = glob.glob(f\"{img_dir}/*.jpg\")\n",
    "    if COCO_IMG_LIMIT:\n",
    "        imgs = random.sample(imgs, min(COCO_IMG_LIMIT, len(imgs)))\n",
    "    for img_path in tqdm(imgs, desc=f\"COCO {split}\"):\n",
    "        base = os.path.splitext(os.path.basename(img_path))[0]\n",
    "        lbl_path = os.path.join(lbl_dir, base + \".txt\")\n",
    "        if not os.path.exists(lbl_path): continue\n",
    "        new_lines = []\n",
    "        with open(lbl_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                arr = line.strip().split()\n",
    "                if not arr: continue\n",
    "                cls = int(arr[0])\n",
    "                if cls in COCO_TO_MASTER:\n",
    "                    new_cls = COCO_TO_MASTER[cls]\n",
    "                    # Ensure we only use the 4 bounding box values, ignoring segmentation data\n",
    "                    new_lines.append(\" \".join([str(new_cls)] + arr[1:5]))\n",
    "        if new_lines:\n",
    "            out_img = f\"{BASE_DIR}/images/{split}/{base}.jpg\"\n",
    "            out_lbl = f\"{BASE_DIR}/labels/{split}/{base}.txt\"\n",
    "            shutil.copy(img_path, out_img)\n",
    "            with open(out_lbl, \"w\") as f:\n",
    "                f.write(\"\\n\".join(new_lines))\n",
    "\n",
    "print(\"COCO dataset processed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7RXqoXicClE_"
   },
   "outputs": [],
   "source": [
    "# @title Cell 6: OpenImages dataset download\n",
    "\n",
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "import fiftyone.utils.openimages as fouo\n",
    "\n",
    "# 1. Full valid OI class set\n",
    "oi_valid_classes = set(fouo.get_classes())\n",
    "\n",
    "# 2. Define desired yard/obstacle classes\n",
    "OI_YARD_CLASSES = [\n",
    "    \"Flower\", \"Fountain\", \"Stairs\", \"Person\", \"Dog\", \"Cat\", \"Bicycle\",\n",
    "    \"Car\", \"Bus\", \"Truck\", \"Motorcycle\", \"Bench\", \"Tree\", \"Lamp\", \"Wheelchair\", \"Table\", \"Chair\",\n",
    "]\n",
    "\n",
    "# 3. Only request OI classes that exist\n",
    "oi_classes_to_load = [c for c in OI_YARD_CLASSES if c in oi_valid_classes]\n",
    "print(\"Attempting to load from OpenImages:\", oi_classes_to_load)\n",
    "\n",
    "# 4. Download data\n",
    "oi_dataset = foz.load_zoo_dataset(\n",
    "    \"open-images-v6\",\n",
    "    split=\"train\",\n",
    "    label_types=[\"detections\"],\n",
    "    classes=oi_classes_to_load,\n",
    "    max_samples=2500,  # Increased sample size for more variety\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "# 5. Export to YOLO format for processing\n",
    "oi_dataset.export(\n",
    "    export_dir=\"/content/fo_openimages_yolo\",\n",
    "    dataset_type=fo.types.YOLOv5Dataset\n",
    ")\n",
    "print(\"OpenImages export complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "new_cell_for_oi_processing"
   },
   "outputs": [],
   "source": [
    "# @title Cell 7: Process and Integrate OpenImages Data\n",
    "import yaml\n",
    "\n",
    "print(\"Processing and integrating OpenImages dataset...\")\n",
    "\n",
    "# Directory where fiftyone exported the data\n",
    "oi_export_dir = \"/content/fo_openimages_yolo\"\n",
    "oi_img_dir = f\"{oi_export_dir}/data/images\"\n",
    "oi_lbl_dir = f\"{oi_export_dir}/data/labels\"\n",
    "\n",
    "# Check if the export directory exists\n",
    "if not os.path.isdir(oi_export_dir):\n",
    "    print(\"OpenImages export directory not found. Skipping integration.\")\n",
    "else:\n",
    "    # Read the class mapping from the exported dataset.yaml\n",
    "    with open(f\"{oi_export_dir}/dataset.yaml\", 'r') as f:\n",
    "        oi_yaml = yaml.safe_load(f)\n",
    "    oi_names = oi_yaml['names']\n",
    "\n",
    "    # Create a mapping from OI class name to our master class index\n",
    "    OI_NAME_TO_MASTER = {\n",
    "        \"Tree\": master_index[\"tree\"],\n",
    "        \"Flower\": master_index[\"flower\"],\n",
    "        \"Person\": master_index[\"person\"],\n",
    "        \"Car\": master_index[\"car\"],\n",
    "        \"Bus\": master_index[\"bus\"],\n",
    "        \"Truck\": master_index[\"truck\"],\n",
    "        \"Bicycle\": master_index[\"bicycle\"],\n",
    "        \"Cat\": master_index[\"cat\"],\n",
    "        \"Dog\": master_index[\"dog\"],\n",
    "        \"Stairs\": master_index[\"stairs\"],\n",
    "        \"Fountain\": master_index[\"fountain\"],\n",
    "        \"Bench\": master_index[\"bench\"],\n",
    "        \"Lamp\": master_index[\"lamp_post\"],\n",
    "        \"Chair\": master_index[\"furniture\"],\n",
    "        \"Table\": master_index[\"furniture\"],\n",
    "        \"Motorcycle\": master_index[\"vehicle\"],\n",
    "        \"Wheelchair\": master_index[\"vehicle\"]\n",
    "    }\n",
    "\n",
    "    # Map from the numeric index in the OI export to our master index\n",
    "    oi_idx_to_master_idx = {\n",
    "        oi_idx: OI_NAME_TO_MASTER.get(name)\n",
    "        for oi_idx, name in enumerate(oi_names)\n",
    "        if OI_NAME_TO_MASTER.get(name) is not None\n",
    "    }\n",
    "\n",
    "    # Process and copy the files\n",
    "    imgs = glob.glob(f\"{oi_img_dir}/*.jpg\")\n",
    "    for img_path in tqdm(imgs, desc=\"OpenImages Process\"):\n",
    "        base = os.path.splitext(os.path.basename(img_path))[0]\n",
    "        lbl_path = os.path.join(oi_lbl_dir, base + \".txt\")\n",
    "\n",
    "        if not os.path.exists(lbl_path):\n",
    "            continue\n",
    "\n",
    "        new_lines = []\n",
    "        with open(lbl_path, 'r') as f:\n",
    "            for line in f:\n",
    "                arr = line.strip().split()\n",
    "                if not arr: continue\n",
    "                oi_cls_idx = int(arr[0])\n",
    "                if oi_cls_idx in oi_idx_to_master_idx:\n",
    "                    master_cls_idx = oi_idx_to_master_idx[oi_cls_idx]\n",
    "                    # Ensure we only take the 4 bounding box coordinates\n",
    "                    new_lines.append(f\"{master_cls_idx} \" + \" \".join(arr[1:5]))\n",
    "\n",
    "        if new_lines:\n",
    "            # Copy to train split\n",
    "            out_img = f\"{BASE_DIR}/images/train/oi_{base}.jpg\"\n",
    "            out_lbl = f\"{BASE_DIR}/labels/train/oi_{base}.txt\"\n",
    "            shutil.copy(img_path, out_img)\n",
    "            with open(out_lbl, 'w') as f:\n",
    "                f.write(\"\\n\".join(new_lines))\n",
    "\n",
    "    print(\"OpenImages dataset processed and added to training set!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HeaPzbspCoXk"
   },
   "outputs": [],
   "source": [
    "# @title Cell 8: Roboflow Fence dataset (requires API key)\n",
    "from roboflow import Roboflow\n",
    "from google.colab import userdata\n",
    "ROBOFLOW_API_KEY = userdata.get('ROBOFLOW_API_KEY')\n",
    "rf = Roboflow(api_key=ROBOFLOW_API_KEY)\n",
    "fence_ds = rf.workspace(\"uji-thesis\").project(\"broken-fence-detection\").version(1).download(\"yolov8\", location=f\"{BASE_DIR}/fence\")\n",
    "for split in [\"train\", \"valid\"]:\n",
    "    img_dir = f\"{BASE_DIR}/fence/{split}/images\"\n",
    "    lbl_dir = f\"{BASE_DIR}/fence/{split}/labels\"\n",
    "    imgs = glob.glob(f\"{img_dir}/*.jpg\")\n",
    "    for img_path in tqdm(imgs, desc=f\"Fence {split}\"):\n",
    "        base = os.path.splitext(os.path.basename(img_path))[0]\n",
    "        lbl_path = os.path.join(lbl_dir, base + \".txt\")\n",
    "        if not os.path.exists(lbl_path): continue\n",
    "        new_lines = []\n",
    "        with open(lbl_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                arr = line.strip().split()\n",
    "                if not arr: continue\n",
    "                # Ensure we only use the 4 bounding box values, ignoring segmentation data\n",
    "                new_lines.append(f\"{master_index['fence']} \" + \" \".join(arr[1:5]))\n",
    "        if new_lines:\n",
    "          # Add all Roboflow data to the training set\n",
    "          out_img = f\"{BASE_DIR}/images/train/{base}_rf.jpg\"\n",
    "          out_lbl = f\"{BASE_DIR}/labels/train/{base}_rf.txt\"\n",
    "          shutil.copy(img_path, out_img)\n",
    "          with open(out_lbl, \"w\") as f:\n",
    "              f.write(\"\\n\".join(new_lines))\n",
    "\n",
    "print(\"Fence dataset processed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yn-frZ1sCrBy"
   },
   "outputs": [],
   "source": [
    "# @title Cell 9: ADE20K via Kaggle (awsaf49/ade20k-dataset)\n",
    "if not os.path.exists(\"/content/ade20k-dataset.zip\"):\n",
    "    !kaggle datasets download -d awsaf49/ade20k-dataset -p /content\n",
    "    !unzip -q /content/ade20k-dataset.zip -d /content/ade20k\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ADE20K category mapping for yard/obstacle/terrain, index: master_class\n",
    "ADE_TO_MASTER = {\n",
    "    1: 26,      # wall -> retaining_wall\n",
    "    5: 7,       # tree -> tree\n",
    "    7: 4,       # road, route -> pavement\n",
    "    10: 0,      # grass -> grass\n",
    "    12: 4,      # sidewalk, pavement -> pavement\n",
    "    13: 36,     # person -> person\n",
    "    14: 1,      # earth, ground -> dirt\n",
    "    16: 48,     # table -> furniture\n",
    "    18: 8,      # plant -> shrub\n",
    "    21: 51,     # car -> car\n",
    "    22: 14,     # water -> water_feature\n",
    "    30: 21,     # field -> field\n",
    "    33: 24,     # fence -> fence\n",
    "    35: 12,     # rock, stone -> rock\n",
    "    39: 27,     # railing, rail -> railing\n",
    "    44: 31,     # signboard, sign -> sign\n",
    "    47: 2,      # sand -> sand\n",
    "    53: 30,     # path -> path\n",
    "    54: 29,     # stairs, steps -> stairs\n",
    "    60: 29,     # stairway, staircase -> stairs\n",
    "    61: 18,     # river -> river\n",
    "    62: 28,     # bridge -> bridge\n",
    "    67: 9,      # flower -> flower\n",
    "    69: 13,     # hill -> hill\n",
    "    70: 28,     # bench -> bench\n",
    "    73: 7,      # palm tree -> tree\n",
    "    80: 8,      # shrub -> shrub\n",
    "    81: 52,     # bus -> bus\n",
    "    83: 33,     # light -> lamp_post\n",
    "    84: 53,     # truck -> truck\n",
    "    88: 34,     # streetlight -> streetlight\n",
    "    94: 32,     # pole -> pole\n",
    "    95: 1,      # land, ground, soil -> dirt\n",
    "    105: 19,    # fountain -> fountain\n",
    "    109: 42,    # plaything, toy -> toy\n",
    "    110: 16,    # swimming pool -> pool\n",
    "    114: 20,    # waterfall, falls -> waterfall\n",
    "    127: 37,    # animal -> animal\n",
    "    128: 40,    # bicycle -> bicycle\n",
    "    129: 17,    # lake -> lake\n",
    "    137: 35,    # traffic light -> traffic_light\n",
    "    139: 55,    # ashcan, trash can -> trash_bin\n",
    "}\n",
    "print(\"ADE20K category mapping loaded!\")\n",
    "\n",
    "img_dir = \"/content/ade20k/ADEChallengeData2016/images/training\"\n",
    "mask_dir = \"/content/ade20k/ADEChallengeData2016/annotations/training\"\n",
    "\n",
    "imgs = sorted(glob.glob(f\"{img_dir}/*.jpg\"))\n",
    "masks = sorted(glob.glob(f\"{mask_dir}/*.png\"))\n",
    "\n",
    "print(f\"Found {len(imgs)} images and {len(masks)} masks!\")\n",
    "\n",
    "assert len(imgs) == len(masks), \"Mismatch in image and mask counts!\"\n",
    "\n",
    "print(\"Processing ADE20K images...\")\n",
    "\n",
    "for img_path, mask_path in tqdm(zip(imgs, masks), total=len(imgs), desc=\"ADE20K images\"):\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    mask = np.array(Image.open(mask_path))\n",
    "    W, H = img.size # Correctly get width and height from PIL Image\n",
    "    objs = []\n",
    "    for ade_class, master_id in ADE_TO_MASTER.items():\n",
    "        ys, xs = np.where(mask == ade_class)\n",
    "        if len(xs) < 2 or len(ys) < 2: # Need at least 2 points to form a box\n",
    "            continue\n",
    "        xmin, xmax = xs.min(), xs.max()\n",
    "        ymin, ymax = ys.min(), ys.max()\n",
    "        # Skip zero-area boxes\n",
    "        if xmin >= xmax or ymin >= ymax:\n",
    "            continue\n",
    "        x_c = (xmin + xmax) / 2.0 / W\n",
    "        y_c = (ymin + ymax) / 2.0 / H\n",
    "        bw = (xmax - xmin) / W\n",
    "        bh = (ymax - ymin) / H\n",
    "        objs.append(f\"{master_id} {x_c:.6f} {y_c:.6f} {bw:.6f} {bh:.6f}\")\n",
    "    if objs:\n",
    "        base = os.path.splitext(os.path.basename(img_path))[0]\n",
    "        out_img = f\"{BASE_DIR}/images/train/ade_{base}.jpg\"\n",
    "        out_lbl = f\"{BASE_DIR}/labels/train/ade_{base}.txt\"\n",
    "        img.save(out_img)\n",
    "        with open(out_lbl, \"w\") as f:\n",
    "            f.write(\"\\n\".join(objs))\n",
    "\n",
    "print(\"ADE20K Kaggle dataset processed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r9Km1CAihLW9"
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# @title Cell 10: WRITE DATA CONFIG FILE\n",
    "# ----------------------------\n",
    "with open(f\"{BASE_DIR}/data.yaml\", \"w\") as f:\n",
    "    f.write(f\"path: {BASE_DIR}\\n\")\n",
    "    f.write(\"train: images/train\\n\")\n",
    "    f.write(\"val: images/val\\n\")\n",
    "    f.write(f\"nc: {len(MASTER_CLASS_LIST)}\\n\")\n",
    "    f.write(\"names: \" + str(MASTER_CLASS_LIST) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dyV2xcmzjWX8"
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# @title Cell 11: TRAIN MODELS WITH CHECKPOINT RESUME\n",
    "# ----------------------------\n",
    "import torch\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import glob\n",
    "import gc\n",
    "import re\n",
    "import time # Import time for modification time\n",
    "from tqdm import tqdm # Import tqdm here as it's used later\n",
    "from ultralytics import YOLO\n",
    "from google.colab import drive\n",
    "\n",
    "# Mount Google Drive for automated backups\n",
    "drive.mount('/content/drive')\n",
    "BACKUP_DIR = \"/content/drive/MyDrive/mower_model_checkpoints\"\n",
    "os.makedirs(BACKUP_DIR, exist_ok=True)\n",
    "\n",
    "# --- (IMPROVED) Define the backup callback function ---\n",
    "def backup_checkpoint_callback(trainer):\n",
    "    \"\"\"\n",
    "    A callback to save model-specific checkpoints to Google Drive every 5 epochs.\n",
    "    \"\"\"\n",
    "    epoch = trainer.epoch\n",
    "    # Get the actual run name (which might have suffixes like '_2')\n",
    "    run_name = os.path.basename(trainer.save_dir)\n",
    "    model_name = trainer.args.name # Gets the base model name (e.g., 'pi_model_yolov8n')\n",
    "\n",
    "    # Use the run_name from save_dir, which reflects the actual directory name\n",
    "    backup_file_name = f\"{run_name}_epoch_{epoch+1}.pt\"\n",
    "    src_path = trainer.last\n",
    "    dst_path = os.path.join(BACKUP_DIR, backup_file_name)\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        if os.path.exists(src_path):\n",
    "            try:\n",
    "                shutil.copy2(src_path, dst_path)\n",
    "                print(f\"‚úÖ [Backup] Epoch {epoch+1} for '{run_name}' saved to Google Drive at {dst_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå [Backup] Failed to save checkpoint {backup_file_name} to Google Drive: {e}\")\n",
    "\n",
    "\n",
    "# --- (IMPROVED) Function to find the latest checkpoint by modification date ---\n",
    "def find_latest_checkpoint(backup_dir, base_model_name):\n",
    "    \"\"\"\n",
    "    Finds the latest checkpoint file for a given base model name in the backup directory\n",
    "    based on the file modification date, handling potential suffixes added by the\n",
    "    training framework. Returns the file path or None if no checkpoint is found.\n",
    "    \"\"\"\n",
    "    latest_checkpoint = None\n",
    "    latest_mtime = 0 # Use modification time\n",
    "    # Match files starting with the base name, potentially followed by _\\d+ and _epoch_\\d+\n",
    "    # The epoch number is not used for sorting here, only for filtering by base name\n",
    "    pattern = re.compile(rf\"^{re.escape(base_model_name)}(_\\d+)?_epoch_\\d+\\.pt$\")\n",
    "\n",
    "    if not os.path.exists(backup_dir):\n",
    "        print(f\"Backup directory not found: {backup_dir}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"Searching for latest checkpoint matching pattern '{pattern.pattern}' in {backup_dir} by modification date.\")\n",
    "\n",
    "    matching_files = []\n",
    "    for filename in os.listdir(backup_dir):\n",
    "        if pattern.match(filename):\n",
    "            file_path = os.path.join(backup_dir, filename)\n",
    "            matching_files.append((file_path, os.path.getmtime(file_path)))\n",
    "\n",
    "    if matching_files:\n",
    "        # Find the file with the latest modification time\n",
    "        latest_checkpoint, latest_mtime = max(matching_files, key=lambda item: item[1])\n",
    "        # Optionally, parse the epoch number from the latest file found for reporting\n",
    "        epoch_match = re.search(r\"_epoch_(\\d+)\\.pt$\", os.path.basename(latest_checkpoint))\n",
    "        latest_epoch_str = epoch_match.group(1) if epoch_match else \"N/A\"\n",
    "        print(f\"üîé Found latest checkpoint for '{base_model_name}' (modified {time.ctime(latest_mtime)}): {latest_checkpoint} (Epoch: {latest_epoch_str})\")\n",
    "    else:\n",
    "        print(f\"üëç No existing checkpoint found matching pattern '{pattern.pattern}'. Starting new training.\")\n",
    "\n",
    "    return latest_checkpoint\n",
    "\n",
    "# --- Clear GPU function ---\n",
    "def clear_gpu():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"Cleared GPU cache.\")\n",
    "\n",
    "# --- Auto-create validation set if empty ---\n",
    "# (This part of the code remains unchanged)\n",
    "train_img_dir = f\"{BASE_DIR}/images/train\"\n",
    "val_img_dir = f\"{BASE_DIR}/images/val\"\n",
    "train_lbl_dir = f\"{BASE_DIR}/labels/train\"\n",
    "val_lbl_dir = f\"{BASE_DIR}/labels/val\"\n",
    "\n",
    "# Ensure BASE_DIR is defined (from Cell 2)\n",
    "if 'BASE_DIR' not in globals():\n",
    "    print(\"Error: BASE_DIR is not defined. Please run Cell 2 first.\")\n",
    "else:\n",
    "    if not os.path.exists(val_img_dir) or not os.listdir(val_img_dir):\n",
    "        print(\"Validation set is empty or not found. Creating one...\")\n",
    "        all_imgs = [img for img in os.listdir(train_img_dir) if img.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        # Calculate number of validation images, ensure at least 1 and at most 1500\n",
    "        num_val = min(1500, max(1, len(all_imgs) // 10))\n",
    "        if len(all_imgs) > num_val:\n",
    "            val_imgs_to_move = random.sample(all_imgs, num_val)\n",
    "\n",
    "            for img_name in tqdm(val_imgs_to_move, desc=\"Moving validation images\"):\n",
    "                src_img = os.path.join(train_img_dir, img_name)\n",
    "                dst_img = os.path.join(val_img_dir, img_name)\n",
    "                shutil.move(src_img, dst_img)\n",
    "\n",
    "                label_name = os.path.splitext(img_name)[0] + \".txt\"\n",
    "                src_lbl = os.path.join(train_lbl_dir, label_name)\n",
    "                dst_lbl = os.path.join(val_lbl_dir, label_name)\n",
    "                if os.path.exists(src_lbl):\n",
    "                    shutil.move(src_lbl, dst_lbl)\n",
    "                elif not os.path.exists(src_lbl):\n",
    "                    # Create an empty label file if it doesn't exist in the source\n",
    "                    # This prevents errors later if the image had no annotations\n",
    "                    with open(dst_lbl, 'w') as f:\n",
    "                        pass # Create an empty file\n",
    "            print(f\"Moved {num_val} images/labels to validation set.\")\n",
    "        else:\n",
    "            print(f\"Not enough images ({len(all_imgs)}) to create a validation set of size {num_val}. Skipping validation set creation.\")\n",
    "    else:\n",
    "        print(\"Validation set already exists and is not empty.\")\n",
    "\n",
    "\n",
    "# --- Train Pi model with resume logic ---\n",
    "pi_model_name = \"pi_model_yolov8n\"\n",
    "# Look for the latest checkpoint by modification date\n",
    "pi_checkpoint_path = find_latest_checkpoint(BACKUP_DIR, pi_model_name)\n",
    "\n",
    "# Load from checkpoint if it exists, otherwise start new\n",
    "total_epochs_pi = 50 # Set your desired total epochs here\n",
    "if pi_checkpoint_path:\n",
    "    print(f\"Attempting to resume Pi model training from {pi_checkpoint_path}.\")\n",
    "else:\n",
    "    print(\"Starting new Pi model training.\")\n",
    "\n",
    "model_pi = YOLO(pi_checkpoint_path) if pi_checkpoint_path else YOLO(\"yolov8n.yaml\")\n",
    "model_pi.add_callback(\"on_train_epoch_end\", backup_checkpoint_callback)\n",
    "\n",
    "# Adjust epochs and resume flag based on whether a checkpoint was found\n",
    "pi_train_args = {\n",
    "    \"data\": f\"{BASE_DIR}/data.yaml\",\n",
    "    \"epochs\": total_epochs_pi, # Set total epochs\n",
    "    \"imgsz\": 640,\n",
    "    \"batch\": -1,\n",
    "    \"workers\": 6,\n",
    "    \"patience\": 10,\n",
    "    \"seed\": 42,\n",
    "    \"project\": \"mower_model\",\n",
    "    \"name\": pi_model_name,\n",
    "    \"resume\": bool(pi_checkpoint_path) # Explicitly tell trainer to resume\n",
    "}\n",
    "\n",
    "# Only set device if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    pi_train_args[\"device\"] = 0 # Use GPU 0\n",
    "\n",
    "model_pi.train(**pi_train_args)\n",
    "\n",
    "\n",
    "clear_gpu()\n",
    "del model_pi\n",
    "gc.collect()\n",
    "\n",
    "# --- Train Coral model with resume logic ---\n",
    "coral_model_name = \"coral_model_yolov8n\"\n",
    "# Look for the latest checkpoint by modification date\n",
    "coral_checkpoint_path = find_latest_checkpoint(BACKUP_DIR, coral_model_name)\n",
    "\n",
    "# Load from checkpoint if it exists, otherwise start new\n",
    "total_epochs_coral = 50 # Set your desired total epochs here\n",
    "if coral_checkpoint_path:\n",
    "     print(f\"Attempting to resume Coral model training from {coral_checkpoint_path}.\")\n",
    "else:\n",
    "    print(\"Starting new Coral model training.\")\n",
    "\n",
    "model_coral = YOLO(coral_checkpoint_path) if coral_checkpoint_path else YOLO(\"yolov8n.yaml\")\n",
    "model_coral.add_callback(\"on_train_epoch_end\", backup_checkpoint_callback)\n",
    "\n",
    "# Adjust epochs and resume flag based on whether a checkpoint was found\n",
    "coral_train_args = {\n",
    "    \"data\": f\"{BASE_DIR}/data.yaml\",\n",
    "    \"epochs\": total_epochs_coral, # Set total epochs\n",
    "    \"imgsz\": 640,\n",
    "    \"batch\": -1,\n",
    "    \"workers\": 6,\n",
    "    \"patience\": 10,\n",
    "    \"seed\": 42,\n",
    "    \"project\": \"mower_model\",\n",
    "    \"name\": coral_model_name,\n",
    "    \"resume\": bool(coral_checkpoint_path) # Explicitly tell trainer to resume\n",
    "}\n",
    "\n",
    "# Only set device if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    coral_train_args[\"device\"] = 0 # Use GPU 0\n",
    "\n",
    "model_coral.train(**coral_train_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_AO2zOOe9j_H"
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# @title Cell 12: CREATE REPRESENTATIVE SET\n",
    "# ----------------------------\n",
    "rep_data_dir = \"/content/valid_images/\"\n",
    "os.makedirs(rep_data_dir, exist_ok=True)\n",
    "val_imgs = glob.glob(os.path.join(f\"{BASE_DIR}/images/val\", \"*.jpg\"))\n",
    "for img_path in random.sample(val_imgs, min(150, len(val_imgs))):\n",
    "    shutil.copy(img_path, rep_data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wEJ84WIb9hat"
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# @title Cell 13: EXPORT MODELS (with GPU Check)\n",
    "# ----------------------------\n",
    "import tensorflow as tf\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "# --- (NEW) GPU VERIFICATION STEP ---\n",
    "print(\"Verifying GPU availability...\")\n",
    "!nvidia-smi\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "if gpu_devices:\n",
    "    print(f\"‚úÖ TensorFlow has detected the following GPUs: {gpu_devices}\")\n",
    "else:\n",
    "    print(\"‚ùå WARNING: TensorFlow did NOT detect a GPU. The export will be very slow.\")\n",
    "# ---\n",
    "\n",
    "# Save label file\n",
    "with open(\"labels.txt\", \"w\") as f:\n",
    "    for name in MASTER_CLASS_LIST:\n",
    "        f.write(name + \"\\n\")\n",
    "\n",
    "# Define the base path where training results are saved\n",
    "train_output_base = \"mower_model\"\n",
    "\n",
    "# --- Export TFLite for Pi model ---\n",
    "pi_model_path = os.path.join(train_output_base, \"pi_model_yolov8n\", \"weights\", \"best.pt\")\n",
    "model_pi = YOLO(pi_model_path)\n",
    "model_pi.export(format=\"tflite\", imgsz=640, int8=False, name=\"pi_best_float32\")\n",
    "\n",
    "# --- Export TFLite and ONNX for Coral model ---\n",
    "coral_model_path = os.path.join(train_output_base, \"coral_model_yolov8n\", \"weights\", \"best.pt\")\n",
    "model_coral = YOLO(coral_model_path)\n",
    "\n",
    "# --- Create a temporary YAML file for INT8 quantization ---\n",
    "rep_yaml_path = \"/content/rep_data.yaml\"\n",
    "with open(f\"{BASE_DIR}/data.yaml\", 'r') as f:\n",
    "    data_config = yaml.safe_load(f)\n",
    "data_config['val'] = rep_data_dir\n",
    "with open(rep_yaml_path, 'w') as f:\n",
    "    yaml.dump(data_config, f)\n",
    "# ---\n",
    "\n",
    "# Correctly export the quantized TFLite model for Coral\n",
    "print(f\"Starting INT8 TFLite export using configuration from '{rep_yaml_path}'...\")\n",
    "model_coral.export(\n",
    "    format=\"tflite\",\n",
    "    imgsz=640,\n",
    "    int8=True,\n",
    "    data=rep_yaml_path,\n",
    "    name=\"coral_best_int8\"\n",
    ")\n",
    "\n",
    "# Export the ONNX model\n",
    "model_coral.export(format=\"onnx\", imgsz=640, name=\"coral_best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NtJw0SiP9exy"
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# @title Cell 14: COMPILE FOR CORAL\n",
    "# ----------------------------\n",
    "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n",
    "!echo \"deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\" | sudo tee /etc/apt/sources.list.d/coral-edgetpu.list\n",
    "!sudo apt-get update && sudo apt-get install edgetpu-compiler -y\n",
    "!edgetpu_compiler /content/mower_model/coral_model_yolov8n/weights/coral_best_int8.tflite --out_dir=/content/autonomous_mower/models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NWE3lLnT9bWp"
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# @title Cell 15: PUSH TO GITHUB\n",
    "# ----------------------------\n",
    "from google.colab import userdata\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "GITHUB_REPO_URL = \"https://github.com/acredsfan/autonomous_mower.git\"\n",
    "GITHUB_BRANCH = \"code_rebuild\"\n",
    "# Get the GitHub Personal Access Token from Colab secrets\n",
    "GITHUB_PAT = userdata.get('GITHUB_PAT')\n",
    "\n",
    "if not GITHUB_PAT:\n",
    "    print(\"Error: GitHub Personal Access Token not found in secrets. Please add it as 'GITHUB_PAT'.\")\n",
    "else:\n",
    "    !git config --global user.email \"you@example.com\"\n",
    "    !git config --global user.name \"Autonomous Mower Trainer\"\n",
    "\n",
    "    # Clone the repository using the PAT\n",
    "    !git clone -b $GITHUB_BRANCH https://$GITHUB_PAT@github.com/acredsfan/autonomous_mower.git push_dir\n",
    "\n",
    "    # Check if the clone was successful before proceeding\n",
    "    if os.path.exists(\"push_dir\"):\n",
    "        # Copy the generated files to the cloned repository\n",
    "        !cp -r pi_best_float32.tflite coral_best_int8.tflite coral_best_edgetpu.tflite coral_best.onnx labels.txt push_dir/models/\n",
    "\n",
    "        # Change directory to the cloned repository\n",
    "        %cd push_dir\n",
    "\n",
    "        # Add, commit, and push the changes\n",
    "        !git add models/*\n",
    "        !git commit -m \"Add full dataset trained YOLOv8 models for Pi and Coral, compiled for EdgeTPU\"\n",
    "        !git push origin $GITHUB_BRANCH\n",
    "\n",
    "        # Change back to the original directory\n",
    "        %cd ..\n",
    "    else:\n",
    "        print(\"Error: Repository cloning failed. Please check your PAT and repository URL.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "__sBDRlqX7pl"
   },
   "outputs": [],
   "source": [
    "# prompt: I want the final models saved to my Google Drive folder\n",
    "\n",
    "# @title Cell 16: SAVE MODELS TO GOOGLE DRIVE\n",
    "# ----------------------------\n",
    "\n",
    "# Define the destination folder in your Google Drive\n",
    "DRIVE_SAVE_DIR = \"/content/drive/MyDrive/Mower_Trained_Models\"\n",
    "os.makedirs(DRIVE_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Define the files to save from the current Colab environment\n",
    "files_to_save = [\n",
    "    \"pi_best_float32.tflite\",\n",
    "    \"coral_best_int8.tflite\",\n",
    "    \"coral_best_edgetpu.tflite\",\n",
    "    \"coral_best.onnx\",\n",
    "    \"labels.txt\"\n",
    "]\n",
    "\n",
    "print(f\"Saving models to Google Drive folder: {DRIVE_SAVE_DIR}\")\n",
    "\n",
    "for file_name in files_to_save:\n",
    "    src_path = file_name\n",
    "    dst_path = os.path.join(DRIVE_SAVE_DIR, file_name)\n",
    "    if os.path.exists(src_path):\n",
    "        shutil.copy2(src_path, dst_path)\n",
    "        print(f\"‚úÖ Saved {file_name} to Google Drive\")\n",
    "    else:\n",
    "        print(f\"‚ùå Error: {file_name} not found. Skipping save to Google Drive.\")\n",
    "\n",
    "print(\"Model saving to Google Drive complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNcZdO6kdDvSxMn1WVZXVep",
   "gpuType": "A100",
   "include_colab_link": true,
   "machine_shape": "hm",
   "mount_file_id": "1A1pmzVh5vRuj9dDcFjOUkff8jdt0jRV0",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
