{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 1: Final Combined Installation and Verification\n",
        "# This is the ONLY cell you need for installations.\n",
        "\n",
        "# 1. Uninstall to ensure a clean slate\n",
        "print(\"Uninstalling all relevant libraries...\")\n",
        "!pip uninstall -y torch torchvision torchaudio ultralytics fiftyone tflite-runtime roboflow datasets\n",
        "\n",
        "# 2. Install GPU-enabled PyTorch\n",
        "print(\"\\nInstalling GPU-enabled PyTorch...\")\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
        "\n",
        "# 3. Install all other dependencies in one go\n",
        "print(\"\\nInstalling Ultralytics and other libraries...\")\n",
        "!pip install ultralytics roboflow fiftyone datasets tflite-runtime --quiet\n",
        "\n",
        "# 4. Verify the installation\n",
        "print(\"\\nVerifying the environment...\")\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Is CUDA available? {torch.cuda.is_available()}\")\n",
        "\n",
        "print(\"\\nRunning Ultralytics checks:\")\n",
        "import ultralytics\n",
        "ultralytics.checks()"
      ],
      "metadata": {
        "id": "cLD-8eVnL364"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 2: Class Setup and Directory Initialization\n",
        "# ----------------------------\n",
        "# CLASS SETUP & DIRECTORY INIT\n",
        "# ----------------------------\n",
        "import os\n",
        "MASTER_CLASS_LIST = [\"grass\", \"dirt\", \"sand\", \"mulch\", \"pavement\", \"concrete\", \"gravel\", \"tree\", \"shrub\", \"flower\", \"planter\", \"stump\", \"rock\", \"hill\", \"water_feature\", \"ditch\", \"pool\", \"lake\", \"river\", \"fountain\", \"waterfall\", \"field\", \"curb\", \"edging\", \"fence\", \"gate\", \"retaining_wall\", \"railing\", \"bench\", \"bridge\", \"stairs\", \"path\", \"sign\", \"pole\", \"lamp_post\", \"streetlight\", \"traffic_light\", \"person\", \"animal\", \"dog\", \"cat\", \"bicycle\", \"toy\", \"tool\", \"hose\", \"sprinkler\", \"swing_set\", \"slide\", \"sandbox\", \"trampoline\", \"furniture\", \"decoration\", \"vehicle\", \"car\", \"bus\", \"truck\", \"mailbox\", \"trash_bin\", \"recycling_bin\"]\n",
        "master_index = {name: idx for idx, name in enumerate(MASTER_CLASS_LIST)}\n",
        "BASE_DIR = \"/content/mower_dataset\"\n",
        "for split in [\"train\", \"val\"]:\n",
        "    os.makedirs(f\"{BASE_DIR}/images/{split}\", exist_ok=True)\n",
        "    os.makedirs(f\"{BASE_DIR}/labels/{split}\", exist_ok=True)\n"
      ],
      "metadata": {
        "id": "_bKiBYGHm_er"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LyYiEQyeCYV2"
      },
      "outputs": [],
      "source": [
        "# @title Cell 3: Kaggle setup for COCO\n",
        "from google.colab import drive\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the path to kaggle.json in your Google Drive\n",
        "kaggle_drive_path = \"/content/drive/MyDrive/kaggle.json\"\n",
        "kaggle_colab_path = \"/root/.kaggle/kaggle.json\"\n",
        "\n",
        "# Create the .kaggle directory if it doesn't exist\n",
        "if not os.path.exists(\"/root/.kaggle\"):\n",
        "    os.makedirs(\"/root/.kaggle\")\n",
        "\n",
        "# Check if the file exists in Google Drive before copying\n",
        "if os.path.exists(kaggle_drive_path):\n",
        "    shutil.copy(kaggle_drive_path, kaggle_colab_path)\n",
        "    os.chmod(kaggle_colab_path, 0o600)\n",
        "    print(\"kaggle.json copied from Google Drive.\")\n",
        "else:\n",
        "    print(f\"Error: {kaggle_drive_path} not found in your Google Drive.\")\n",
        "    print(\"Please upload kaggle.json to the root of your MyDrive or manually upload it.\")\n",
        "    # Fallback to manual upload if the file is not in Drive\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()  # upload kaggle.json\n",
        "    for fn in uploaded.keys():\n",
        "        shutil.move(fn, kaggle_colab_path)\n",
        "    os.chmod(kaggle_colab_path, 0o600)\n",
        "    print(\"kaggle.json uploaded manually.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtzFimoOCbXK"
      },
      "outputs": [],
      "source": [
        "# @title Cell 4: Download COCO 2017 (YOLOv8 format from Kaggle)\n",
        "!kaggle datasets download -d paragmraw/coco-2017-dataset-yolov8-format -p /content\n",
        "!unzip -q /content/coco-2017-dataset-yolov8-format.zip -d /content/coco2017\n",
        "print(\"COCO 2017 dataset downloaded and extracted!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CF4n-PzsCeaq"
      },
      "outputs": [],
      "source": [
        "# @title Cell 5: COCO mapping and filtering\n",
        "COCO_TO_MASTER = {\n",
        "    0: 36,    # person\n",
        "    1: 40,    # bicycle\n",
        "    2: 51,    # car\n",
        "    3: 50,    # motorcycle -> vehicle\n",
        "    5: 52,    # bus\n",
        "    7: 53,    # truck\n",
        "    15: 39,   # cat\n",
        "    16: 38,   # dog\n",
        "    57: 48,   # chair -> furniture\n",
        "    60: 48,   # dining table -> furniture\n",
        "    58: 9,    # potted plant -> planter\n",
        "    77: 42,   # teddy bear -> toy\n",
        "}\n",
        "COCO_IMG_LIMIT = None  # Adjust for RAM/time; set to None for full dataset\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Ensure BASE_DIR is defined (from Cell 2)\n",
        "if 'BASE_DIR' not in globals():\n",
        "    print(\"Error: BASE_DIR is not defined. Please run Cell 2 first.\")\n",
        "\n",
        "for split in [\"train\", \"val\"]:\n",
        "    img_dir = f\"/content/coco2017/{split}/images\"\n",
        "    lbl_dir = f\"/content/coco2017/{split}/labels\"\n",
        "    imgs = glob.glob(f\"{img_dir}/*.jpg\")\n",
        "    if COCO_IMG_LIMIT:\n",
        "        imgs = random.sample(imgs, min(COCO_IMG_LIMIT, len(imgs)))\n",
        "    for img_path in tqdm(imgs, desc=f\"COCO {split}\"):\n",
        "        base = os.path.splitext(os.path.basename(img_path))[0]\n",
        "        lbl_path = os.path.join(lbl_dir, base + \".txt\")\n",
        "        if not os.path.exists(lbl_path): continue\n",
        "        new_lines = []\n",
        "        with open(lbl_path, \"r\") as f:\n",
        "            for line in f:\n",
        "                arr = line.strip().split()\n",
        "                if not arr: continue\n",
        "                cls = int(arr[0])\n",
        "                if cls in COCO_TO_MASTER:\n",
        "                    new_cls = COCO_TO_MASTER[cls]\n",
        "                    # Ensure we only use the 4 bounding box values, ignoring segmentation data\n",
        "                    new_lines.append(\" \".join([str(new_cls)] + arr[1:5]))\n",
        "        if new_lines:\n",
        "            out_img = f\"{BASE_DIR}/images/{split}/{base}.jpg\"\n",
        "            out_lbl = f\"{BASE_DIR}/labels/{split}/{base}.txt\"\n",
        "            shutil.copy(img_path, out_img)\n",
        "            with open(out_lbl, \"w\") as f:\n",
        "                f.write(\"\\n\".join(new_lines))\n",
        "\n",
        "print(\"COCO dataset processed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7RXqoXicClE_"
      },
      "outputs": [],
      "source": [
        "# @title Cell 6: OpenImages dataset download\n",
        "\n",
        "import fiftyone as fo\n",
        "import fiftyone.zoo as foz\n",
        "import fiftyone.utils.openimages as fouo\n",
        "\n",
        "# 1. Full valid OI class set\n",
        "oi_valid_classes = set(fouo.get_classes())\n",
        "\n",
        "# 2. Define desired yard/obstacle classes\n",
        "OI_YARD_CLASSES = [\n",
        "    \"Flower\", \"Fountain\", \"Stairs\", \"Person\", \"Dog\", \"Cat\", \"Bicycle\",\n",
        "    \"Car\", \"Bus\", \"Truck\", \"Motorcycle\", \"Bench\", \"Tree\", \"Lamp\", \"Wheelchair\", \"Table\", \"Chair\",\n",
        "]\n",
        "\n",
        "# 3. Only request OI classes that exist\n",
        "oi_classes_to_load = [c for c in OI_YARD_CLASSES if c in oi_valid_classes]\n",
        "print(\"Attempting to load from OpenImages:\", oi_classes_to_load)\n",
        "\n",
        "# 4. Download data\n",
        "oi_dataset = foz.load_zoo_dataset(\n",
        "    \"open-images-v6\",\n",
        "    split=\"train\",\n",
        "    label_types=[\"detections\"],\n",
        "    classes=oi_classes_to_load,\n",
        "    max_samples=2500,  # Increased sample size for more variety\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "# 5. Export to YOLO format for processing\n",
        "oi_dataset.export(\n",
        "    export_dir=\"/content/fo_openimages_yolo\",\n",
        "    dataset_type=fo.types.YOLOv5Dataset\n",
        ")\n",
        "print(\"OpenImages export complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "new_cell_for_oi_processing"
      },
      "outputs": [],
      "source": [
        "# @title Cell 7: Process and Integrate OpenImages Data\n",
        "import yaml\n",
        "\n",
        "print(\"Processing and integrating OpenImages dataset...\")\n",
        "\n",
        "# Directory where fiftyone exported the data\n",
        "oi_export_dir = \"/content/fo_openimages_yolo\"\n",
        "oi_img_dir = f\"{oi_export_dir}/data/images\"\n",
        "oi_lbl_dir = f\"{oi_export_dir}/data/labels\"\n",
        "\n",
        "# Check if the export directory exists\n",
        "if not os.path.isdir(oi_export_dir):\n",
        "    print(\"OpenImages export directory not found. Skipping integration.\")\n",
        "else:\n",
        "    # Read the class mapping from the exported dataset.yaml\n",
        "    with open(f\"{oi_export_dir}/dataset.yaml\", 'r') as f:\n",
        "        oi_yaml = yaml.safe_load(f)\n",
        "    oi_names = oi_yaml['names']\n",
        "\n",
        "    # Create a mapping from OI class name to our master class index\n",
        "    OI_NAME_TO_MASTER = {\n",
        "        \"Tree\": master_index[\"tree\"],\n",
        "        \"Flower\": master_index[\"flower\"],\n",
        "        \"Person\": master_index[\"person\"],\n",
        "        \"Car\": master_index[\"car\"],\n",
        "        \"Bus\": master_index[\"bus\"],\n",
        "        \"Truck\": master_index[\"truck\"],\n",
        "        \"Bicycle\": master_index[\"bicycle\"],\n",
        "        \"Cat\": master_index[\"cat\"],\n",
        "        \"Dog\": master_index[\"dog\"],\n",
        "        \"Stairs\": master_index[\"stairs\"],\n",
        "        \"Fountain\": master_index[\"fountain\"],\n",
        "        \"Bench\": master_index[\"bench\"],\n",
        "        \"Lamp\": master_index[\"lamp_post\"],\n",
        "        \"Chair\": master_index[\"furniture\"],\n",
        "        \"Table\": master_index[\"furniture\"],\n",
        "        \"Motorcycle\": master_index[\"vehicle\"],\n",
        "        \"Wheelchair\": master_index[\"vehicle\"]\n",
        "    }\n",
        "\n",
        "    # Map from the numeric index in the OI export to our master index\n",
        "    oi_idx_to_master_idx = {\n",
        "        oi_idx: OI_NAME_TO_MASTER.get(name)\n",
        "        for oi_idx, name in enumerate(oi_names)\n",
        "        if OI_NAME_TO_MASTER.get(name) is not None\n",
        "    }\n",
        "\n",
        "    # Process and copy the files\n",
        "    imgs = glob.glob(f\"{oi_img_dir}/*.jpg\")\n",
        "    for img_path in tqdm(imgs, desc=\"OpenImages Process\"):\n",
        "        base = os.path.splitext(os.path.basename(img_path))[0]\n",
        "        lbl_path = os.path.join(oi_lbl_dir, base + \".txt\")\n",
        "\n",
        "        if not os.path.exists(lbl_path):\n",
        "            continue\n",
        "\n",
        "        new_lines = []\n",
        "        with open(lbl_path, 'r') as f:\n",
        "            for line in f:\n",
        "                arr = line.strip().split()\n",
        "                if not arr: continue\n",
        "                oi_cls_idx = int(arr[0])\n",
        "                if oi_cls_idx in oi_idx_to_master_idx:\n",
        "                    master_cls_idx = oi_idx_to_master_idx[oi_cls_idx]\n",
        "                    # Ensure we only take the 4 bounding box coordinates\n",
        "                    new_lines.append(f\"{master_cls_idx} \" + \" \".join(arr[1:5]))\n",
        "\n",
        "        if new_lines:\n",
        "            # Copy to train split\n",
        "            out_img = f\"{BASE_DIR}/images/train/oi_{base}.jpg\"\n",
        "            out_lbl = f\"{BASE_DIR}/labels/train/oi_{base}.txt\"\n",
        "            shutil.copy(img_path, out_img)\n",
        "            with open(out_lbl, 'w') as f:\n",
        "                f.write(\"\\n\".join(new_lines))\n",
        "\n",
        "    print(\"OpenImages dataset processed and added to training set!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HeaPzbspCoXk"
      },
      "outputs": [],
      "source": [
        "# @title Cell 8: Roboflow Fence dataset (requires API key)\n",
        "from roboflow import Roboflow\n",
        "from google.colab import userdata\n",
        "ROBOFLOW_API_KEY = userdata.get('ROBOFLOW_API_KEY')\n",
        "rf = Roboflow(api_key=ROBOFLOW_API_KEY)\n",
        "fence_ds = rf.workspace(\"uji-thesis\").project(\"broken-fence-detection\").version(1).download(\"yolov8\", location=f\"{BASE_DIR}/fence\")\n",
        "for split in [\"train\", \"valid\"]:\n",
        "    img_dir = f\"{BASE_DIR}/fence/{split}/images\"\n",
        "    lbl_dir = f\"{BASE_DIR}/fence/{split}/labels\"\n",
        "    imgs = glob.glob(f\"{img_dir}/*.jpg\")\n",
        "    for img_path in tqdm(imgs, desc=f\"Fence {split}\"):\n",
        "        base = os.path.splitext(os.path.basename(img_path))[0]\n",
        "        lbl_path = os.path.join(lbl_dir, base + \".txt\")\n",
        "        if not os.path.exists(lbl_path): continue\n",
        "        new_lines = []\n",
        "        with open(lbl_path, \"r\") as f:\n",
        "            for line in f:\n",
        "                arr = line.strip().split()\n",
        "                if not arr: continue\n",
        "                # Ensure we only use the 4 bounding box values, ignoring segmentation data\n",
        "                new_lines.append(f\"{master_index['fence']} \" + \" \".join(arr[1:5]))\n",
        "        if new_lines:\n",
        "          # Add all Roboflow data to the training set\n",
        "          out_img = f\"{BASE_DIR}/images/train/{base}_rf.jpg\"\n",
        "          out_lbl = f\"{BASE_DIR}/labels/train/{base}_rf.txt\"\n",
        "          shutil.copy(img_path, out_img)\n",
        "          with open(out_lbl, \"w\") as f:\n",
        "              f.write(\"\\n\".join(new_lines))\n",
        "\n",
        "print(\"Fence dataset processed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yn-frZ1sCrBy"
      },
      "outputs": [],
      "source": [
        "# @title Cell 9: ADE20K via Kaggle (awsaf49/ade20k-dataset)\n",
        "if not os.path.exists(\"/content/ade20k-dataset.zip\"):\n",
        "    !kaggle datasets download -d awsaf49/ade20k-dataset -p /content\n",
        "    !unzip -q /content/ade20k-dataset.zip -d /content/ade20k\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ADE20K category mapping for yard/obstacle/terrain, index: master_class\n",
        "ADE_TO_MASTER = {\n",
        "    1: 26,      # wall -> retaining_wall\n",
        "    5: 7,       # tree -> tree\n",
        "    7: 4,       # road, route -> pavement\n",
        "    10: 0,      # grass -> grass\n",
        "    12: 4,      # sidewalk, pavement -> pavement\n",
        "    13: 36,     # person -> person\n",
        "    14: 1,      # earth, ground -> dirt\n",
        "    16: 48,     # table -> furniture\n",
        "    18: 8,      # plant -> shrub\n",
        "    21: 51,     # car -> car\n",
        "    22: 14,     # water -> water_feature\n",
        "    30: 21,     # field -> field\n",
        "    33: 24,     # fence -> fence\n",
        "    35: 12,     # rock, stone -> rock\n",
        "    39: 27,     # railing, rail -> railing\n",
        "    44: 31,     # signboard, sign -> sign\n",
        "    47: 2,      # sand -> sand\n",
        "    53: 30,     # path -> path\n",
        "    54: 29,     # stairs, steps -> stairs\n",
        "    60: 29,     # stairway, staircase -> stairs\n",
        "    61: 18,     # river -> river\n",
        "    62: 28,     # bridge -> bridge\n",
        "    67: 9,      # flower -> flower\n",
        "    69: 13,     # hill -> hill\n",
        "    70: 28,     # bench -> bench\n",
        "    73: 7,      # palm tree -> tree\n",
        "    80: 8,      # shrub -> shrub\n",
        "    81: 52,     # bus -> bus\n",
        "    83: 33,     # light -> lamp_post\n",
        "    84: 53,     # truck -> truck\n",
        "    88: 34,     # streetlight -> streetlight\n",
        "    94: 32,     # pole -> pole\n",
        "    95: 1,      # land, ground, soil -> dirt\n",
        "    105: 19,    # fountain -> fountain\n",
        "    109: 42,    # plaything, toy -> toy\n",
        "    110: 16,    # swimming pool -> pool\n",
        "    114: 20,    # waterfall, falls -> waterfall\n",
        "    127: 37,    # animal -> animal\n",
        "    128: 40,    # bicycle -> bicycle\n",
        "    129: 17,    # lake -> lake\n",
        "    137: 35,    # traffic light -> traffic_light\n",
        "    139: 55,    # ashcan, trash can -> trash_bin\n",
        "}\n",
        "print(\"ADE20K category mapping loaded!\")\n",
        "\n",
        "img_dir = \"/content/ade20k/ADEChallengeData2016/images/training\"\n",
        "mask_dir = \"/content/ade20k/ADEChallengeData2016/annotations/training\"\n",
        "\n",
        "imgs = sorted(glob.glob(f\"{img_dir}/*.jpg\"))\n",
        "masks = sorted(glob.glob(f\"{mask_dir}/*.png\"))\n",
        "\n",
        "print(f\"Found {len(imgs)} images and {len(masks)} masks!\")\n",
        "\n",
        "assert len(imgs) == len(masks), \"Mismatch in image and mask counts!\"\n",
        "\n",
        "print(\"Processing ADE20K images...\")\n",
        "\n",
        "for img_path, mask_path in tqdm(zip(imgs, masks), total=len(imgs), desc=\"ADE20K images\"):\n",
        "    img = Image.open(img_path).convert(\"RGB\")\n",
        "    mask = np.array(Image.open(mask_path))\n",
        "    W, H = img.size # Correctly get width and height from PIL Image\n",
        "    objs = []\n",
        "    for ade_class, master_id in ADE_TO_MASTER.items():\n",
        "        ys, xs = np.where(mask == ade_class)\n",
        "        if len(xs) < 2 or len(ys) < 2: # Need at least 2 points to form a box\n",
        "            continue\n",
        "        xmin, xmax = xs.min(), xs.max()\n",
        "        ymin, ymax = ys.min(), ys.max()\n",
        "        # Skip zero-area boxes\n",
        "        if xmin >= xmax or ymin >= ymax:\n",
        "            continue\n",
        "        x_c = (xmin + xmax) / 2.0 / W\n",
        "        y_c = (ymin + ymax) / 2.0 / H\n",
        "        bw = (xmax - xmin) / W\n",
        "        bh = (ymax - ymin) / H\n",
        "        objs.append(f\"{master_id} {x_c:.6f} {y_c:.6f} {bw:.6f} {bh:.6f}\")\n",
        "    if objs:\n",
        "        base = os.path.splitext(os.path.basename(img_path))[0]\n",
        "        out_img = f\"{BASE_DIR}/images/train/ade_{base}.jpg\"\n",
        "        out_lbl = f\"{BASE_DIR}/labels/train/ade_{base}.txt\"\n",
        "        img.save(out_img)\n",
        "        with open(out_lbl, \"w\") as f:\n",
        "            f.write(\"\\n\".join(objs))\n",
        "\n",
        "print(\"ADE20K Kaggle dataset processed!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# @title Cell 10: WRITE DATA CONFIG FILE\n",
        "# ----------------------------\n",
        "with open(f\"{BASE_DIR}/data.yaml\", \"w\") as f:\n",
        "    f.write(f\"path: {BASE_DIR}\\n\")\n",
        "    f.write(\"train: images/train\\n\")\n",
        "    f.write(\"val: images/val\\n\")\n",
        "    f.write(f\"nc: {len(MASTER_CLASS_LIST)}\\n\")\n",
        "    f.write(\"names: \" + str(MASTER_CLASS_LIST) + \"\\n\")\n",
        "    print(f\"data.yaml saved at {BASE_DIR}/data.yaml\")\n"
      ],
      "metadata": {
        "id": "r9Km1CAihLW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 10a: Save all files to Google Drive for resuming later if session ends\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import zipfile\n",
        "import time # Import time for timestamp\n",
        "\n",
        "# Function to save all files in BASE_DIR to Google Drive\n",
        "def save_to_drive():\n",
        "    drive_backup_dir = \"/content/drive/MyDrive/mower_dataset_backup\"\n",
        "    timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "    zip_filename = f\"mower_dataset_backup_{timestamp}.zip\"\n",
        "    temp_zip_path = f\"/tmp/{zip_filename}\" # Save zip to a temporary location\n",
        "    print(f\"Creating zip archive of {BASE_DIR} at {temp_zip_path}...\")\n",
        "\n",
        "    try:\n",
        "        # Create the zip file\n",
        "        with zipfile.ZipFile(temp_zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "            for root, dirs, files in os.walk(BASE_DIR):\n",
        "                # Adjust the root path in the archive so it doesn't include /content/\n",
        "                arcname = os.path.relpath(root, BASE_DIR)\n",
        "                if arcname != '.': # Don't add the base directory itself\n",
        "                    zipf.write(root, arcname)\n",
        "                for file in files:\n",
        "                    file_path = os.path.join(root, file)\n",
        "                    arcname = os.path.relpath(file_path, BASE_DIR)\n",
        "                    zipf.write(file_path, arcname)\n",
        "        print(\"Zip archive created successfully.\")\n",
        "\n",
        "        # Ensure the target directory exists in Drive\n",
        "        os.makedirs(drive_backup_dir, exist_ok=True)\n",
        "        drive_dst_path = os.path.join(drive_backup_dir, zip_filename)\n",
        "\n",
        "        # Copy the zip file to Google Drive\n",
        "        print(f\"Copying zip archive to Google Drive at {drive_dst_path}...\")\n",
        "        shutil.copy2(temp_zip_path, drive_dst_path)\n",
        "        print(\"Data successfully saved to Google Drive.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving data to Google Drive: {e}\")\n",
        "\n",
        "    finally:\n",
        "        # Clean up the temporary zip file\n",
        "        if os.path.exists(temp_zip_path):\n",
        "            os.remove(temp_zip_path)\n",
        "            print(f\"Removed temporary zip file: {temp_zip_path}\")\n",
        "\n",
        "# Call the function to save\n",
        "# Ensure BASE_DIR is defined (e.g., from Cell 2)\n",
        "if 'BASE_DIR' in globals():\n",
        "    save_to_drive()\n",
        "else:\n",
        "    print(\"Error: BASE_DIR is not defined. Please run Cell 2 first.\")"
      ],
      "metadata": {
        "id": "q8DOlsopa9_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 10b: Recover saved backup from Google Drive if resuming later\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import zipfile\n",
        "from google.colab import drive\n",
        "import glob\n",
        "import time # Import time for sorting by modification date\n",
        "\n",
        "# Function to recover data from Google Drive backup\n",
        "def recover_from_drive():\n",
        "    # Ensure Google Drive is mounted\n",
        "    print(\"Checking Google Drive mount status...\")\n",
        "    if not os.path.exists('/content/drive'):\n",
        "        print(\"Google Drive not mounted. Attempting to mount...\")\n",
        "        try:\n",
        "            drive.mount('/content/drive')\n",
        "            print(\"Google Drive mounted successfully.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error mounting Google Drive: {e}\")\n",
        "            print(\"Cannot proceed with recovery without Google Drive mounted.\")\n",
        "            return # Exit function if mount fails\n",
        "    else:\n",
        "        print(\"Google Drive is already mounted.\")\n",
        "\n",
        "    drive_backup_dir = \"/content/drive/MyDrive/mower_dataset_backup\"\n",
        "    target_base_dir = \"/content/mower_dataset\" # This should match BASE_DIR\n",
        "\n",
        "    print(f\"Attempting to recover data from zip backup in {drive_backup_dir}...\")\n",
        "\n",
        "    if not os.path.exists(drive_backup_dir):\n",
        "        print(f\"Backup directory not found at {drive_backup_dir}. Skipping recovery.\")\n",
        "        print(\"If this is your first run or you haven't saved a backup, this is expected.\")\n",
        "        return\n",
        "\n",
        "    # Find the latest zip file based on modification time\n",
        "    list_of_files = glob.glob(f\"{drive_backup_dir}/mower_dataset_backup_*.zip\")\n",
        "    if not list_of_files:\n",
        "        print(f\"No zip backup files found in {drive_backup_dir}. Skipping recovery.\")\n",
        "        return\n",
        "\n",
        "    latest_zip_path = max(list_of_files, key=os.path.getmtime)\n",
        "    print(f\"Found latest backup zip: {latest_zip_path}\")\n",
        "\n",
        "    temp_zip_path = \"/tmp/latest_mower_dataset_backup.zip\"\n",
        "\n",
        "    try:\n",
        "        # Copy the zip file to a temporary location in Colab\n",
        "        print(f\"Copying zip file to temporary location: {temp_zip_path}...\")\n",
        "        shutil.copy2(latest_zip_path, temp_zip_path)\n",
        "        print(\"Zip file copied successfully.\")\n",
        "\n",
        "        # Remove existing directory if it exists to ensure a clean unzip\n",
        "        if os.path.exists(target_base_dir):\n",
        "            print(f\"Removing existing directory: {target_base_dir}\")\n",
        "            shutil.rmtree(target_base_dir)\n",
        "\n",
        "        # Ensure the target directory exists for unzipping\n",
        "        os.makedirs(target_base_dir, exist_ok=True)\n",
        "        print(f\"Extracting zip archive to {target_base_dir}...\")\n",
        "\n",
        "        # Extract the zip file\n",
        "        with zipfile.ZipFile(temp_zip_path, 'r') as zipf:\n",
        "            zipf.extractall(target_base_dir)\n",
        "        print(\"Data successfully recovered and extracted.\")\n",
        "\n",
        "        # Optionally, verify some files exist\n",
        "        if os.path.exists(f\"{target_base_dir}/data.yaml\"):\n",
        "            print(\"Verified data.yaml exists in recovered directory.\")\n",
        "        else:\n",
        "             print(\"Warning: data.yaml not found in the recovered directory.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error recovering data from Google Drive: {e}\")\n",
        "\n",
        "    finally:\n",
        "        # Clean up the temporary zip file\n",
        "        if os.path.exists(temp_zip_path):\n",
        "            os.remove(temp_zip_path)\n",
        "            print(f\"Removed temporary zip file: {temp_zip_path}\")\n",
        "\n",
        "# Call the function to attempt recovery\n",
        "recover_from_drive()\n",
        "\n",
        "# Ensure BASE_DIR variable is set after potential recovery\n",
        "# If recover_from_drive ran, BASE_DIR might have been created/overwritten.\n",
        "# If not, we need to ensure it's defined for subsequent cells.\n",
        "if 'BASE_DIR' not in globals():\n",
        "    BASE_DIR = \"/content/mower_dataset\"\n",
        "    print(f\"BASE_DIR was not set, initializing to {BASE_DIR}\")\n",
        "    # Recreate necessary subdirectories if starting fresh or recovery failed\n",
        "    # This might be redundant if recovery was successful, but safe if not.\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        os.makedirs(f\"{BASE_DIR}/images/{split}\", exist_ok=True)\n",
        "        os.makedirs(f\"{BASE_DIR}/labels/{split}\", exist_ok=True)\n",
        "    print(\"Initialized necessary dataset directories.\")"
      ],
      "metadata": {
        "id": "DPzPTUS9bOwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# @title Cell 11: TRAIN MODELS WITH CHECKPOINT RESUME\n",
        "# ----------------------------\n",
        "import torch\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "import glob\n",
        "import gc\n",
        "import re\n",
        "from tqdm import tqdm # Import tqdm here as it's used later\n",
        "from ultralytics import YOLO\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive for automated backups\n",
        "drive.mount('/content/drive')\n",
        "BACKUP_DIR = \"/content/drive/MyDrive/mower_model_checkpoints\"\n",
        "os.makedirs(BACKUP_DIR, exist_ok=True)\n",
        "\n",
        "# --- (IMPROVED) Define the backup callback function ---\n",
        "def backup_checkpoint_callback(trainer):\n",
        "    \"\"\"\n",
        "    A callback to save model-specific checkpoints to Google Drive every 5 epochs.\n",
        "    \"\"\"\n",
        "    epoch = trainer.epoch\n",
        "    # Get the actual run name (which might have suffixes like '_2')\n",
        "    run_name = os.path.basename(trainer.save_dir)\n",
        "    model_name = trainer.args.name # Gets the base model name (e.g., 'pi_model_yolov8n')\n",
        "\n",
        "    # Use the run_name from save_dir, which reflects the actual directory name\n",
        "    backup_file_name = f\"{run_name}_epoch_{epoch+1}.pt\"\n",
        "    src_path = trainer.last\n",
        "    dst_path = os.path.join(BACKUP_DIR, backup_file_name)\n",
        "\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        if os.path.exists(src_path):\n",
        "            try:\n",
        "                shutil.copy2(src_path, dst_path)\n",
        "                print(f\"✅ [Backup] Epoch {epoch+1} for '{run_name}' saved to Google Drive at {dst_path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"❌ [Backup] Failed to save checkpoint {backup_file_name} to Google Drive: {e}\")\n",
        "\n",
        "\n",
        "# --- (NEW) Function to find the latest checkpoint ---\n",
        "def find_latest_checkpoint(backup_dir, base_model_name):\n",
        "    \"\"\"\n",
        "    Finds the latest checkpoint file for a given base model name in the backup directory,\n",
        "    handling potential suffixes added by the training framework.\n",
        "    Returns the file path or None if no checkpoint is found.\n",
        "    \"\"\"\n",
        "    latest_checkpoint = None\n",
        "    latest_epoch = -1\n",
        "    # Match files starting with the base name, potentially followed by a number suffix\n",
        "    # The suffix can be like 'model_name10' or 'model_name_10' depending on how Ultralytics names runs\n",
        "    # Let's make the regex more flexible to handle both cases\n",
        "    pattern = re.compile(rf\"^{re.escape(base_model_name)}(\\d+)?(_\\d+)?_epoch_(\\d+)\\.pt$\")\n",
        "\n",
        "\n",
        "    if not os.path.exists(backup_dir):\n",
        "        print(f\"Backup directory not found: {backup_dir}\")\n",
        "        return None\n",
        "\n",
        "    print(f\"Searching for checkpoints matching pattern '{pattern.pattern}' in {backup_dir}\")\n",
        "    found_checkpoints = []\n",
        "    for filename in os.listdir(backup_dir):\n",
        "        match = pattern.match(filename)\n",
        "        if match:\n",
        "            # The epoch number is always the last captured group before the extension\n",
        "            epoch = int(match.group(match.lastindex))\n",
        "            found_checkpoints.append((epoch, os.path.join(backup_dir, filename)))\n",
        "\n",
        "    if found_checkpoints:\n",
        "        # Find the checkpoint with the highest epoch number\n",
        "        latest_epoch, latest_checkpoint = max(found_checkpoints, key=lambda item: item[0])\n",
        "        print(f\"🔎 Found latest checkpoint for '{base_model_name}' at epoch {latest_epoch}: {latest_checkpoint}\")\n",
        "    else:\n",
        "        print(f\"👍 No existing checkpoint found matching pattern '{pattern.pattern}'. Starting new training.\")\n",
        "\n",
        "    return latest_checkpoint\n",
        "\n",
        "# --- Clear GPU function ---\n",
        "def clear_gpu():\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    print(\"Cleared GPU cache.\")\n",
        "\n",
        "# --- Auto-create validation set if empty ---\n",
        "# (This part of the code remains unchanged)\n",
        "train_img_dir = f\"{BASE_DIR}/images/train\"\n",
        "val_img_dir = f\"{BASE_DIR}/images/val\"\n",
        "train_lbl_dir = f\"{BASE_DIR}/labels/train\"\n",
        "val_lbl_dir = f\"{BASE_DIR}/labels/val\"\n",
        "\n",
        "# Ensure BASE_DIR is defined (from Cell 2)\n",
        "if 'BASE_DIR' not in globals():\n",
        "    print(\"Error: BASE_DIR is not defined. Please run Cell 2 first.\")\n",
        "else:\n",
        "    if not os.path.exists(val_img_dir) or not os.listdir(val_img_dir):\n",
        "        print(\"Validation set is empty or not found. Creating one...\")\n",
        "        all_imgs = [img for img in os.listdir(train_img_dir) if img.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "        # Calculate number of validation images, ensure at least 1 and at most 1500\n",
        "        num_val = min(1500, max(1, len(all_imgs) // 10))\n",
        "        if len(all_imgs) > num_val:\n",
        "            val_imgs_to_move = random.sample(all_imgs, num_val)\n",
        "\n",
        "            for img_name in tqdm(val_imgs_to_move, desc=\"Moving validation images\"):\n",
        "                src_img = os.path.join(train_img_dir, img_name)\n",
        "                dst_img = os.path.join(val_img_dir, img_name)\n",
        "                shutil.move(src_img, dst_img)\n",
        "\n",
        "                label_name = os.path.splitext(img_name)[0] + \".txt\"\n",
        "                src_lbl = os.path.join(train_lbl_dir, label_name)\n",
        "                dst_lbl = os.path.join(val_lbl_dir, label_name)\n",
        "                if os.path.exists(src_lbl):\n",
        "                    shutil.move(src_lbl, dst_lbl)\n",
        "                elif not os.path.exists(src_lbl):\n",
        "                    # Create an empty label file if it doesn't exist in the source\n",
        "                    # This prevents errors later if the image had no annotations\n",
        "                    with open(dst_lbl, 'w') as f:\n",
        "                        pass # Create an empty file\n",
        "            print(f\"Moved {num_val} images/labels to validation set.\")\n",
        "        else:\n",
        "            print(f\"Not enough images ({len(all_imgs)}) to create a validation set of size {num_val}. Skipping validation set creation.\")\n",
        "    else:\n",
        "        print(\"Validation set already exists and is not empty.\")\n",
        "\n",
        "\n",
        "# --- Train Pi model with resume logic ---\n",
        "clear_gpu()\n",
        "pi_model_name = \"pi_model_yolov8n\"\n",
        "# Look for a checkpoint starting with the base name\n",
        "pi_checkpoint_path = find_latest_checkpoint(BACKUP_DIR, pi_model_name)\n",
        "\n",
        "# Load from checkpoint if it exists, otherwise start new\n",
        "# If resuming, get the epoch number from the filename to set correct starting epoch\n",
        "start_epoch_pi = 0\n",
        "if pi_checkpoint_path:\n",
        "    # Extract epoch number from the found checkpoint path\n",
        "    match = re.search(rf\"{re.escape(pi_model_name)}(\\d+)?(_\\d+)?_epoch_(\\d+)\\.pt$\", pi_checkpoint_path)\n",
        "    if match:\n",
        "        start_epoch_pi = int(match.group(match.lastindex))\n",
        "        # Ultralytics resume starts from the *next* epoch, so set total epochs accordingly\n",
        "        # If we want to train for a total of 50 epochs, and we resume from epoch 25,\n",
        "        # we need to train for 50 - 25 = 25 more epochs.\n",
        "        # The `epochs` argument in train() is the *total* number of epochs to run.\n",
        "        # So if we resume from start_epoch_pi, we want to run until epoch 50 (or more if user desires).\n",
        "        # Let's aim for a total of 50 epochs for now, which means if we resume from epoch X,\n",
        "        # we train X+1 to 50. The train function handles the starting epoch when resume=True.\n",
        "        total_epochs_pi = 50 # Set your desired total epochs here\n",
        "        print(f\"Resuming Pi model training from epoch {start_epoch_pi}. Total epochs set to {total_epochs_pi}.\")\n",
        "    else:\n",
        "         print(f\"Could not parse epoch from checkpoint filename: {pi_checkpoint_path}. Starting new training.\")\n",
        "         pi_checkpoint_path = None # Treat as no valid checkpoint\n",
        "         total_epochs_pi = 50 # Default total epochs for new training\n",
        "\n",
        "model_pi = YOLO(pi_checkpoint_path) if pi_checkpoint_path else YOLO(\"yolov8n.yaml\")\n",
        "model_pi.add_callback(\"on_train_epoch_end\", backup_checkpoint_callback)\n",
        "\n",
        "# Adjust epochs and resume flag based on whether a checkpoint was found\n",
        "pi_train_args = {\n",
        "    \"data\": f\"{BASE_DIR}/data.yaml\",\n",
        "    \"epochs\": total_epochs_pi if pi_checkpoint_path else 50, # Set total epochs\n",
        "    \"imgsz\": 640,\n",
        "    \"batch\": .7,\n",
        "    \"workers\": 6,\n",
        "    \"patience\": 10,\n",
        "    \"seed\": 42,\n",
        "    \"project\": \"mower_model\",\n",
        "    \"name\": pi_model_name,\n",
        "    \"resume\": bool(pi_checkpoint_path) # Explicitly tell trainer to resume\n",
        "}\n",
        "\n",
        "# Only set device if CUDA is available\n",
        "if torch.cuda.is_available():\n",
        "    pi_train_args[\"device\"] = 0 # Use GPU 0\n",
        "\n",
        "model_pi.train(**pi_train_args)\n",
        "\n",
        "\n",
        "clear_gpu()\n",
        "del model_pi\n",
        "gc.collect()\n",
        "\n",
        "# --- Train Coral model with resume logic ---\n",
        "coral_model_name = \"coral_model_yolov8n\"\n",
        "# Look for a checkpoint starting with the base name\n",
        "coral_checkpoint_path = find_latest_checkpoint(BACKUP_DIR, coral_model_name)\n",
        "\n",
        "# Load from checkpoint if it exists, otherwise start new\n",
        "start_epoch_coral = 0\n",
        "if coral_checkpoint_path:\n",
        "     # Extract epoch number from the found checkpoint path\n",
        "    match = re.search(rf\"{re.escape(coral_model_name)}(\\d+)?(_\\d+)?_epoch_(\\d+)\\.pt$\", coral_checkpoint_path)\n",
        "    if match:\n",
        "        start_epoch_coral = int(match.group(match.lastindex))\n",
        "        total_epochs_coral = 50 # Set your desired total epochs here\n",
        "        print(f\"Resuming Coral model training from epoch {start_epoch_coral}. Total epochs set to {total_epochs_coral}.\")\n",
        "    else:\n",
        "         print(f\"Could not parse epoch from checkpoint filename: {coral_checkpoint_path}. Starting new training.\")\n",
        "         coral_checkpoint_path = None # Treat as no valid checkpoint\n",
        "         total_epochs_coral = 50 # Default total epochs for new training\n",
        "\n",
        "\n",
        "model_coral = YOLO(coral_checkpoint_path) if coral_checkpoint_path else YOLO(\"yolov8n.yaml\")\n",
        "model_coral.add_callback(\"on_train_epoch_end\", backup_checkpoint_callback)\n",
        "\n",
        "# Adjust epochs and resume flag based on whether a checkpoint was found\n",
        "coral_train_args = {\n",
        "    \"data\": f\"{BASE_DIR}/data.yaml\",\n",
        "    \"epochs\": total_epochs_coral if coral_checkpoint_path else 50, # Set total epochs\n",
        "    \"imgsz\": 640,\n",
        "    \"batch\": .7,\n",
        "    \"workers\": 6,\n",
        "    \"patience\": 10,\n",
        "    \"seed\": 42,\n",
        "    \"project\": \"mower_model\",\n",
        "    \"name\": coral_model_name,\n",
        "    \"resume\": bool(coral_checkpoint_path) # Explicitly tell trainer to resume\n",
        "}\n",
        "\n",
        "# Only set device if CUDA is available\n",
        "if torch.cuda.is_available():\n",
        "    coral_train_args[\"device\"] = 0 # Use GPU 0\n",
        "\n",
        "model_coral.train(**coral_train_args)"
      ],
      "metadata": {
        "id": "dyV2xcmzjWX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 12: Back up only necessary files to Export Models for Notebook 2\n",
        "\n",
        "import time\n",
        "\n",
        "# --- New Cell: Backup mower_dataset and mower_model folders to Google Drive ---\n",
        "\n",
        "def backup_specific_folders_to_drive():\n",
        "    \"\"\"\n",
        "    Zips and backs up the mower_dataset and mower_model folders to Google Drive.\n",
        "    \"\"\"\n",
        "    # Ensure Google Drive is mounted\n",
        "    print(\"Checking Google Drive mount status...\")\n",
        "    if not os.path.exists('/content/drive'):\n",
        "        print(\"Google Drive not mounted. Attempting to mount...\")\n",
        "        try:\n",
        "            drive.mount('/content/drive')\n",
        "            print(\"Google Drive mounted successfully.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error mounting Google Drive: {e}\")\n",
        "            print(\"Cannot proceed with backup without Google Drive mounted.\")\n",
        "            return # Exit function if mount fails\n",
        "    else:\n",
        "        print(\"Google Drive is already mounted.\")\n",
        "\n",
        "    drive_backup_dir_base = \"/content/drive/MyDrive/mower_backups\" # Use a dedicated backup dir\n",
        "    timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "    zip_filename = f\"mower_backup_{timestamp}.zip\"\n",
        "    temp_zip_path = f\"/tmp/{zip_filename}\" # Save zip to a temporary location\n",
        "    print(f\"Creating zip archive at {temp_zip_path}...\")\n",
        "\n",
        "    # Define the folders to back up (relative to /content)\n",
        "    folders_to_backup = [\"mower_dataset\", \"mower_model\"]\n",
        "\n",
        "    try:\n",
        "        # Create the zip file\n",
        "        with zipfile.ZipFile(temp_zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "            for folder_name in folders_to_backup:\n",
        "                folder_path = f\"/content/{folder_name}\"\n",
        "                if os.path.exists(folder_path):\n",
        "                    print(f\"Adding folder '{folder_name}' to zip...\")\n",
        "                    for root, dirs, files in os.walk(folder_path):\n",
        "                        # Create the path in the zip archive relative to the base folder name\n",
        "                        arcname_root = os.path.join(folder_name, os.path.relpath(root, folder_path))\n",
        "                        # Ensure the directory structure is added\n",
        "                        if arcname_root != folder_name: # Avoid adding the base folder itself twice\n",
        "                             zipf.write(root, arcname_root)\n",
        "\n",
        "                        for file in files:\n",
        "                            file_path = os.path.join(root, file)\n",
        "                            arcname_file = os.path.join(folder_name, os.path.relpath(file_path, folder_path))\n",
        "                            zipf.write(file_path, arcname_file)\n",
        "                else:\n",
        "                    print(f\"Warning: Folder '{folder_name}' not found at {folder_path}. Skipping.\")\n",
        "\n",
        "        print(\"Zip archive created successfully.\")\n",
        "\n",
        "        # Ensure the target directory exists in Drive\n",
        "        os.makedirs(drive_backup_dir_base, exist_ok=True)\n",
        "        drive_dst_path = os.path.join(drive_backup_dir_base, zip_filename)\n",
        "\n",
        "        # Copy the zip file to Google Drive\n",
        "        print(f\"Copying zip archive to Google Drive at {drive_dst_path}...\")\n",
        "        shutil.copy2(temp_zip_path, drive_dst_path)\n",
        "        print(\"Specific folders successfully saved to Google Drive.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving specific folders to Google Drive: {e}\")\n",
        "\n",
        "    finally:\n",
        "        # Clean up the temporary zip file\n",
        "        if os.path.exists(temp_zip_path):\n",
        "            os.remove(temp_zip_path)\n",
        "            print(f\"Removed temporary zip file: {temp_zip_path}\")\n",
        "\n",
        "# Call the function to perform the backup\n",
        "backup_specific_folders_to_drive()\n",
        "\n"
      ],
      "metadata": {
        "id": "H2UY6oi0u_UY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}