{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WcKcl5UxV0O"
      },
      "outputs": [],
      "source": [
        "# @title Cell 1: Restore workspace from backup of Notebook 1 to Export (<1 Minute)\n",
        "from google.colab import drive\n",
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "import zipfile\n",
        "import random\n",
        "\n",
        "# Function to restore workspace from the latest backup\n",
        "def restore_workspace_backup():\n",
        "    # Ensure Google Drive is mounted\n",
        "    print(\"Checking Google Drive mount status...\")\n",
        "    if not os.path.exists('/content/drive'):\n",
        "        print(\"Google Drive not mounted. Attempting to mount...\")\n",
        "        try:\n",
        "            drive.mount('/content/drive')\n",
        "            print(\"Google Drive mounted successfully.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error mounting Google Drive: {e}\")\n",
        "            print(\"Cannot proceed with restore without Google Drive mounted.\")\n",
        "            return # Exit function if mount fails\n",
        "    else:\n",
        "        print(\"Google Drive is already mounted.\")\n",
        "\n",
        "    drive_backup_dir = \"/content/drive/MyDrive/mower_backups\"\n",
        "    print(f\"Attempting to restore workspace from zip backup in {drive_backup_dir}...\")\n",
        "\n",
        "    if not os.path.exists(drive_backup_dir):\n",
        "        print(f\"Backup directory not found at {drive_backup_dir}. Skipping restore.\")\n",
        "        print(\"If this is your first run or you haven't saved a backup, this is expected.\")\n",
        "        return\n",
        "\n",
        "    # Find the latest zip file based on modification time\n",
        "    list_of_files = glob.glob(f\"{drive_backup_dir}/mower_backup_*.zip\")\n",
        "    if not list_of_files:\n",
        "        print(f\"No workspace backup zip files found in {drive_backup_dir}. Skipping restore.\")\n",
        "        return\n",
        "\n",
        "    latest_zip_path = max(list_of_files, key=os.path.getmtime)\n",
        "    print(f\"Found latest workspace backup zip: {latest_zip_path}\")\n",
        "\n",
        "    temp_zip_path = \"/tmp/latest_workspace_backup.zip\"\n",
        "\n",
        "    try:\n",
        "        # Copy the zip file to a temporary location in Colab\n",
        "        print(f\"Copying zip file to temporary location: {temp_zip_path}...\")\n",
        "        shutil.copy2(latest_zip_path, temp_zip_path)\n",
        "        print(\"Zip file copied successfully.\")\n",
        "\n",
        "        # Ensure the target directory exists for unzipping (root /content)\n",
        "        os.makedirs('/content', exist_ok=True)\n",
        "        print(\"Extracting zip archive to /content...\")\n",
        "\n",
        "        # Extract the zip file\n",
        "        # WARNING: This will overwrite existing files in /content that are in the zip!\n",
        "        with zipfile.ZipFile(temp_zip_path, 'r') as zipf:\n",
        "            # Get list of files in the zip to handle potential exclusions during restore if needed\n",
        "            # For a full workspace restore, we typically just extract everything.\n",
        "            zipf.extractall('/content')\n",
        "\n",
        "        print(\"Workspace successfully restored.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error restoring workspace from Google Drive: {e}\")\n",
        "\n",
        "    finally:\n",
        "        # Clean up the temporary zip file\n",
        "        if os.path.exists(temp_zip_path):\n",
        "            os.remove(temp_zip_path)\n",
        "            print(f\"Removed temporary zip file: {temp_zip_path}\")\n",
        "\n",
        "# Call the function to attempt restore\n",
        "restore_workspace_backup()\n",
        "\n",
        "# --- After restoring, you might need to re-verify essential paths/variables ---\n",
        "# This is important because the restoration replaces the /content filesystem state.\n",
        "\n",
        "# Re-define BASE_DIR and MASTER_CLASS_LIST from Cell 2, as they might be needed\n",
        "# immediately by subsequent cells, and the restored environment might not have them defined yet\n",
        "# if this cell is run after a crash and before running Cell 2 again.\n",
        "# It's safest to re-declare them here if they are critical global variables.\n",
        "# If they were defined in Cell 2 which you intend to re-run anyway, this might be redundant.\n",
        "# Assuming BASE_DIR and MASTER_CLASS_LIST were critical for subsequent steps:\n",
        "if 'BASE_DIR' not in globals() or BASE_DIR is None:\n",
        "     BASE_DIR = \"/content/mower_dataset\"\n",
        "     print(f\"Re-initialized BASE_DIR to {BASE_DIR} after restore.\")\n",
        "\n",
        "if 'MASTER_CLASS_LIST' not in globals() or MASTER_CLASS_LIST is None:\n",
        "    MASTER_CLASS_LIST = [\"grass\", \"dirt\", \"sand\", \"mulch\", \"pavement\", \"concrete\", \"gravel\", \"tree\", \"shrub\", \"flower\", \"planter\", \"stump\", \"rock\", \"hill\", \"water_feature\", \"ditch\", \"pool\", \"lake\", \"river\", \"fountain\", \"waterfall\", \"field\", \"curb\", \"edging\", \"fence\", \"gate\", \"retaining_wall\", \"railing\", \"bench\", \"bridge\", \"stairs\", \"path\", \"sign\", \"pole\", \"lamp_post\", \"streetlight\", \"traffic_light\", \"person\", \"animal\", \"dog\", \"cat\", \"bicycle\", \"toy\", \"tool\", \"hose\", \"sprinkler\", \"swing_set\", \"slide\", \"sandbox\", \"trampoline\", \"furniture\", \"decoration\", \"vehicle\", \"car\", \"bus\", \"truck\", \"mailbox\", \"trash_bin\", \"recycling_bin\"]\n",
        "    master_index = {name: idx for idx, name in enumerate(MASTER_CLASS_LIST)} # Re-create index\n",
        "    print(\"Re-initialized MASTER_CLASS_LIST and master_index after restore.\")\n",
        "\n",
        "# You might also need to re-load the data.yaml file path if it was critical\n",
        "# This is already handled by the `recover_from_drive` function in Cell 10b,\n",
        "# but if you skipped 10b, you might need something like:\n",
        "# DATA_YAML_PATH = f\"{BASE_DIR}/data.yaml\"\n",
        "# print(f\"Set DATA_YAML_PATH to {DATA_YAML_PATH}\")\n",
        "\n",
        "# You might also need to remount Drive explicitly if restore didn't handle it,\n",
        "# though the restore function attempts to do so.\n",
        "# drive.mount('/content/drive', force_remount=True) # Use force_remount if needed\n",
        "\n",
        "# After running this cell, you should then proceed to the cell where the export was attempted\n",
        "# (likely the cell after 11a which triggered the need for 11b)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 1b: Create labels.txt\n",
        "\n",
        "print(\"Creating labels.txt file...\")\n",
        "\n",
        "# This list should be defined in the previous \"Restore\" cell\n",
        "if 'MASTER_CLASS_LIST' in globals():\n",
        "    with open(\"labels.txt\", \"w\") as f:\n",
        "        for class_name in MASTER_CLASS_LIST:\n",
        "            f.write(class_name + \"\\n\")\n",
        "    print(\"✅ labels.txt created successfully at /content/labels.txt\")\n",
        "else:\n",
        "    print(\"❌ ERROR: MASTER_CLASS_LIST is not defined. Cannot create labels.txt.\")"
      ],
      "metadata": {
        "id": "QOqL95MDzIZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHg_iO9V0-H6"
      },
      "outputs": [],
      "source": [
        "# @title Cell 2: Final Setup for Model Export (~3 Minutes)\n",
        "\n",
        "# 1. Install GPU-enabled PyTorch\n",
        "print(\"Installing GPU-enabled PyTorch...\")\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124 --quiet\n",
        "\n",
        "# 2. Install TensorFlow, Ultralytics, AND all specific export dependencies\n",
        "# This adds 'ai-edge-litert' to preempt the final AutoUpdate trigger.\n",
        "print(\"\\nInstalling TF, Ultralytics, and ALL export requirements...\")\n",
        "!pip install tensorflow ultralytics 'sng4onnx>=1.0.1' 'onnx_graphsurgeon>=0.3.26' 'onnx>=1.12.0,<1.18.0' 'onnx2tf>=1.26.3' 'onnxslim>=0.1.59' onnx-simplifier onnxruntime-gpu 'ai-edge-litert>=1.2.0,<1.4.0' --quiet\n",
        "\n",
        "# 3. Verify the environment\n",
        "print(\"\\nVerifying the environment...\")\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Is CUDA available? {torch.cuda.is_available()}\")\n",
        "\n",
        "# 4. Install the Edge TPU Compiler\n",
        "print(\"\\nInstalling Edge TPU Compiler...\")\n",
        "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n",
        "!echo \"deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\" | sudo tee /etc/apt/sources.list.d/coral-edgetpu.list\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install edgetpu-compiler -y\n",
        "print(\"✅ Edge TPU Compiler installed.\")\n",
        "\n",
        "# Run the official Ultralytics check\n",
        "print(\"\\nRunning Ultralytics checks:\")\n",
        "import ultralytics\n",
        "ultralytics.checks()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 3a: (Optional) Force GPU Usage with Environment Variable\n",
        "# Run this BEFORE the export cells if GPU still isn't being used\n",
        "\n",
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
        "\n",
        "# Also set TensorFlow to use GPU memory growth\n",
        "import tensorflow as tf\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(f\"TensorFlow GPU memory growth enabled for {len(gpus)} GPU(s)\")\n",
        "    except RuntimeError as e:\n",
        "        print(e)"
      ],
      "metadata": {
        "id": "suES6KcpI3wF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEJ84WIb9hat"
      },
      "outputs": [],
      "source": [
        "# @title Cell 3: Export Models and Compile for Coral\n",
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "import random\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# --- 1. Create Representative Dataset ---\n",
        "print(\"--- Creating Representative Dataset ---\")\n",
        "rep_data_dir = \"/content/valid_images/\"\n",
        "os.makedirs(rep_data_dir, exist_ok=True)\n",
        "BASE_DIR = \"/content/mower_dataset\"\n",
        "val_imgs = glob.glob(os.path.join(f\"{BASE_DIR}/images/val\", \"*.jpg\"))\n",
        "if val_imgs:\n",
        "    # Use a fixed random seed for reproducibility\n",
        "    random.seed(42)\n",
        "    for img_path in random.sample(val_imgs, min(150, len(val_imgs))):\n",
        "        shutil.copy(img_path, rep_data_dir)\n",
        "    print(f\"Created representative dataset with {len(os.listdir(rep_data_dir))} images.\")\n",
        "else:\n",
        "    print(\"WARNING: Validation image folder is empty.\")\n",
        "\n",
        "# --- 2. Export Pi Model (Float32 TFLite) ---\n",
        "print(\"\\n--- Exporting Pi Model ---\")\n",
        "pi_model_pt_path = \"/content/mower_model/pi_model_yolov8n10/weights/best.pt\"\n",
        "model_pi = YOLO(pi_model_pt_path)\n",
        "# Explicitly set output name and format for clarity\n",
        "model_pi.export(format=\"tflite\", imgsz=640, int8=False, name=\"pi_model_float32\", device=0)\n",
        "print(\"✅ Pi model export complete.\")\n",
        "\n",
        "# --- 3. Export Coral Model to Simplified ONNX ---\n",
        "print(\"\\n--- Exporting Coral Model to Simplified ONNX ---\")\n",
        "coral_model_pt_path = \"/content/mower_model/coral_model_yolov8n/weights/best.pt\"\n",
        "coral_onnx_output_path = \"/content/mower_model/coral_model_yolov8n/weights/best.onnx\"\n",
        "\n",
        "model_coral = YOLO(coral_model_pt_path)\n",
        "# Export with nms=False to simplify the graph for the Edge TPU compiler\n",
        "model_coral.export(format=\"onnx\", imgsz=640, nms=False, name=\"coral_model_simplified\", device=0)\n",
        "print(f\"✅ Coral model simplified ONNX export complete: {coral_onnx_output_path}\") # Note: Ultralytics saves ONNX to working dir by default with 'name'\n",
        "\n",
        "# --- Diagnostic: Check onnx2tf help ---\n",
        "# print(\"\\n--- Diagnostic: onnx2tf help ---\")\n",
        "# !onnx2tf --help\n",
        "# print(\"--- End of Diagnostic ---\")\n",
        "# --- End of Diagnostic ---\n",
        "\n",
        "\n",
        "# --- 4. Convert Simplified ONNX to INT8 Quantized TFLite ---\n",
        "print(\"\\n--- Converting Simplified ONNX to INT8 TFLite ---\")\n",
        "quantized_tflite_path = \"/content/coral_model_quantized_int8.tflite\"\n",
        "# Explicitly define input and output paths in the command\n",
        "# Using --quantization=int8 and --dataset-for-calibration based on onnx2tf --help\n",
        "!onnx2tf -i {coral_onnx_output_path} -o {quantized_tflite_path} -oiqt\n",
        "\n",
        "# # --- Diagnostic: Check file system after onnx2tf ---\n",
        "# print(\"\\n--- Diagnostic: Checking file system after onnx2tf ---\")\n",
        "# print(f\"Current working directory: {os.getcwd()}\")\n",
        "# print(f\"Listing files in /content/ after onnx2tf:\")\n",
        "# !ls /content/\n",
        "# print(\"\\nRecursively searching for .tflite files in /content/ (excluding /content/drive):\")\n",
        "# !find /content/ -name \"*.tflite\" -not -path \"/content/drive/*\"\n",
        "# print(\"--- End of Diagnostic ---\")\n",
        "# # --- End of Diagnostic ---\n",
        "\n",
        "\n",
        "print(\"✅ ONNX to INT8 TFLite conversion command executed.\")\n",
        "\n",
        "# --- 5. Compile Quantized TFLite for Edge TPU ---\n",
        "print(f\"--- Compiling for Edge TPU ---\")\n",
        "output_dir = \"/content/compiled_models/\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# The onnx2tf tool creates a directory. We need to find the correct file inside it.\n",
        "quantized_tflite_dir = \"/content/coral_model_quantized_int8.tflite\"\n",
        "quantized_tflite_src_file = \"\" # Initialize variable\n",
        "\n",
        "# Search for the full integer quantized TFLite file\n",
        "search_pattern = os.path.join(quantized_tflite_dir, \"*_full_integer_quant.tflite\")\n",
        "found_files = glob.glob(search_pattern)\n",
        "\n",
        "if found_files:\n",
        "    quantized_tflite_src_file = found_files[0] # Get the first match\n",
        "    print(f\"Found quantized model for compilation: {quantized_tflite_src_file}\")\n",
        "\n",
        "    # Check if the input file exists and is not empty\n",
        "    if os.path.exists(quantized_tflite_src_file) and os.path.getsize(quantized_tflite_src_file) > 0:\n",
        "        print(f\"Input file for Edge TPU compilation is valid (Size: {os.path.getsize(quantized_tflite_src_file)} bytes)\")\n",
        "        !edgetpu_compiler {quantized_tflite_src_file} --out_dir={output_dir}\n",
        "        print(\"✅ Edge TPU compilation command executed.\")\n",
        "    else:\n",
        "        print(f\"❌ Error: Input file for Edge TPU compilation is not valid: {quantized_tflite_src_file}\")\n",
        "else:\n",
        "    print(f\"❌ Error: No '*_full_integer_quant.tflite' file found in {quantized_tflite_dir}.\")\n",
        "    print(\"This is likely because the onnx2tf conversion failed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWE3lLnT9bWp"
      },
      "outputs": [],
      "source": [
        "# @title Cell 4: PUSH TO GITHUB\n",
        "from google.colab import userdata\n",
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "\n",
        "# --- PUSH TO GITHUB ---\n",
        "print(\"--- PUSHING MODELS TO GITHUB ---\")\n",
        "GITHUB_REPO_URL = \"https://github.com/acredsfan/autonomous_mower.git\"\n",
        "GITHUB_BRANCH = \"main\"\n",
        "GITHUB_PAT = userdata.get('GITHUB_PAT')\n",
        "\n",
        "if not GITHUB_PAT:\n",
        "    print(\"Error: GitHub Personal Access Token not found in secrets.\")\n",
        "    print(\"Skipping GitHub push.\")\n",
        "else:\n",
        "    # Configure Git\n",
        "    !git config --global user.email \"you@example.com\"\n",
        "    !git config --global user.name \"Autonomous Mower Trainer\"\n",
        "\n",
        "    push_dir = \"push_dir\"\n",
        "    if os.path.exists(push_dir) and os.path.isdir(os.path.join(push_dir, '.git')):\n",
        "        print(f\"Repository directory '{push_dir}' already exists. Attempting to pull latest changes...\")\n",
        "        %cd {push_dir}\n",
        "        !git pull origin {GITHUB_BRANCH}\n",
        "        %cd ..\n",
        "    else:\n",
        "        if os.path.exists(push_dir):\n",
        "             print(f\"Removing non-git directory '{push_dir}' before cloning.\")\n",
        "             shutil.rmtree(push_dir)\n",
        "        print(f\"Cloning repository {GITHUB_REPO_URL} branch {GITHUB_BRANCH}...\")\n",
        "        !git clone --depth 1 -b {GITHUB_BRANCH} https://{GITHUB_PAT}@github.com/acredsfan/autonomous_mower.git {push_dir}\n",
        "\n",
        "    if os.path.exists(push_dir) and os.path.isdir(os.path.join(push_dir, '.git')):\n",
        "        # Define source paths based on the actual output of Cell 3\n",
        "        pi_tflite_src = \"/content/mower_model/pi_model_yolov8n10/weights/best_saved_model/best_float32.tflite\"\n",
        "        coral_onnx_src = \"/content/mower_model/coral_model_yolov8n/weights/best.onnx\"\n",
        "        coral_int8_tflite_src = \"/content/coral_model_quantized_int8.tflite/best_full_integer_quant.tflite\"\n",
        "        coral_edgetpu_src = \"/content/compiled_models/best_full_integer_quant_edgetpu.tflite\"\n",
        "\n",
        "        labels_src = \"/content/labels.txt\"\n",
        "\n",
        "        models_dest_dir = os.path.join(push_dir, \"models\")\n",
        "        os.makedirs(models_dest_dir, exist_ok=True)\n",
        "        print(f\"Copying generated files to {models_dest_dir}...\")\n",
        "\n",
        "        files_to_copy = {\n",
        "            pi_tflite_src: \"pi_model_float32.tflite\",\n",
        "            coral_onnx_src: \"coral_model_simplified.onnx\",\n",
        "            coral_int8_tflite_src: \"coral_model_quantized_int8.tflite\",\n",
        "            coral_edgetpu_src: \"coral_model_quantized_int8_edgetpu.tflite\",\n",
        "            labels_src: \"labels.txt\"\n",
        "        }\n",
        "\n",
        "        for src, dest_name in files_to_copy.items():\n",
        "            if src and os.path.exists(src):\n",
        "                dest_path = os.path.join(models_dest_dir, dest_name)\n",
        "                shutil.copy2(src, dest_path)\n",
        "                print(f\"Copied {os.path.basename(src)} to {dest_path}\")\n",
        "            else:\n",
        "                print(f\"WARNING: Source file not found for '{dest_name}'. Path: '{src}'. Skipping.\")\n",
        "\n",
        "        # Commit and push changes\n",
        "        %cd {push_dir}\n",
        "        print(\"Adding files to Git...\")\n",
        "        !git add models/*\n",
        "        # Force add labels.txt if it's being ignored\n",
        "        if os.path.exists(\"models/labels.txt\"):\n",
        "            !git add -f models/labels.txt\n",
        "\n",
        "        git_status = !git status --porcelain\n",
        "        if git_status:\n",
        "            print(\"Committing changes...\")\n",
        "            !git commit -m \"Update trained models and compilation results\"\n",
        "            print(\"Pushing changes to origin...\")\n",
        "            !git push origin {GITHUB_BRANCH}\n",
        "        else:\n",
        "            print(\"No new changes to commit.\")\n",
        "\n",
        "        # Return to the root content directory\n",
        "        %cd ..\n",
        "    else:\n",
        "        print(\"Error: Repository directory was not created/found. Skipping GitHub push.\")\n",
        "\n",
        "print(\"\\nGitHub push process complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__sBDRlqX7pl"
      },
      "outputs": [],
      "source": [
        "# @title Cell 5: SAVE MODELS TO GOOGLE DRIVE\n",
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "\n",
        "# Define the destination folder in your Google Drive\n",
        "DRIVE_SAVE_DIR = \"/content/drive/MyDrive/Mower_Trained_Models\"\n",
        "os.makedirs(DRIVE_SAVE_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"Saving models to Google Drive folder: {DRIVE_SAVE_DIR}\")\n",
        "\n",
        "# Define the source paths for the models generated in Cell 3\n",
        "# These paths are based on the output of the export and compile steps in Cell 3.\n",
        "pi_tflite_src = \"/content/mower_model/pi_model_yolov8n10/weights/best_saved_model/best_float32.tflite\"\n",
        "coral_onnx_src = \"/content/mower_model/coral_model_yolov8n/weights/best.onnx\"\n",
        "coral_int8_tflite_src = \"/content/coral_model_quantized_int8.tflite/best_full_integer_quant.tflite\"\n",
        "coral_edgetpu_src = \"/content/compiled_models/best_full_integer_quant_edgetpu.tflite\"\n",
        "\n",
        "labels_src = \"/content/labels.txt\"\n",
        "\n",
        "models_dest_dir = os.path.join(push_dir, \"models\")\n",
        "os.makedirs(models_dest_dir, exist_ok=True)\n",
        "print(f\"Copying generated files to {models_dest_dir}...\")\n",
        "\n",
        "files_to_copy = {\n",
        "    pi_tflite_src: \"pi_model_float32.tflite\",\n",
        "    coral_onnx_src: \"coral_model_simplified.onnx\",\n",
        "    coral_int8_tflite_src: \"coral_model_quantized_int8.tflite\",\n",
        "    coral_edgetpu_src: \"coral_model_quantized_int8_edgetpu.tflite\",\n",
        "    labels_src: \"labels.txt\"\n",
        "}\n",
        "\n",
        "saved_count = 0\n",
        "for src, dest_name in files_to_copy.items():\n",
        "    if src and os.path.exists(src):\n",
        "        dst_path = os.path.join(DRIVE_SAVE_DIR, dest_name)\n",
        "        try:\n",
        "            shutil.copy2(src, dst_path)\n",
        "            print(f\"✅ Saved {dest_name} to Google Drive\")\n",
        "            saved_count += 1\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error saving {dest_name} to Google Drive: {e}\")\n",
        "    else:\n",
        "        print(f\"❌ Error: Source file not found for Google Drive save: {src}. Skipping.\")\n",
        "\n",
        "print(f\"\\nModel saving to Google Drive complete. {saved_count} file(s) saved.\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}